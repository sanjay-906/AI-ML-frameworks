{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b70985-9dbc-4401-abf5-a51b613f2bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458daac9-a86d-4abc-a1b3-dd8104e95ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "import os\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from keys import gemini_api_key, gemini_api_key2\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from IPython.display import Markdown\n",
    "import databaseinfo as dbi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "import psycopg2\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import TextSplitter\n",
    "import re\n",
    "from typing import List, Any\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "import logging\n",
    "from typing import Generic, Iterator, Sequence, TypeVar\n",
    "from langchain.schema import Document\n",
    "from langchain_core.stores import BaseStore\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from sqlalchemy import inspect\n",
    "from tabulate import tabulate\n",
    "from psycopg2 import sql\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from psycopg2.errors import InsufficientPrivilege\n",
    "import nest_asyncio\n",
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd95368-81f7-475c-b075-bca1b38d9423",
   "metadata": {},
   "source": [
    "## 1. LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd84678-03f0-441c-8fbb-a6fc79696f97",
   "metadata": {},
   "source": [
    "langchain implements a special runnable interface for all of its classes. They all are derived from the class RunnableSeriable.<br>\n",
    "That is why any langchain's method can be put inside lcel chain.<br>\n",
    "When the python interpreter sees the | symbol between 2 objects, it thinks it as the \"or\" operator and attempts to feed a into b:<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2927de6a-c596-46d7-ba74-b8b0b833aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            # the other func consumes the result of this func\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "def add_one(x):\n",
    "    return x+1\n",
    "\n",
    "def multiply_three(x):\n",
    "    return 3*x\n",
    "\n",
    "#make the function runnable\n",
    "a= Runnable(add_one)\n",
    "b= Runnable(multiply_three)\n",
    "\n",
    "chain= a | b\n",
    "ans= chain(42)\n",
    "print(ans)\n",
    "\n",
    "chain= a.__or__(b)   \n",
    "ans= chain(42)\n",
    "print(ans)\n",
    "#both the above act the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c387c-272a-487a-b395-339cd151ce3d",
   "metadata": {},
   "source": [
    "all the components participating in that chain have the \"invoke\"method. The requirement is the datatype of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd54267-175b-448f-b717-8d7216e125fd",
   "metadata": {},
   "source": [
    "## 2. Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e218b5-be31-46a5-ad05-0304fd27732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RunnablePassThrough: it does nothing, just passes whatever input it got\n",
    "#RunnableParallel: creates a dictonary with keys as branches and values as the previous input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "552d83b0-e53d-4a0f-9753-16676a7dfee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= RunnablePassthrough() | RunnablePassthrough() | RunnablePassthrough()\n",
    "chain.invoke(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8240a5dd-b2fd-4b7d-b38f-01d9a5a72a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heyheyhey'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def x3(x):\n",
    "    return x*3\n",
    "\n",
    "chain= RunnablePassthrough() | RunnableLambda(x3) | RunnablePassthrough()\n",
    "chain.invoke(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2311c9de-9b60-4dc4-90e9-2b7543005e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'input1': 'Dawn', 'input2': 'Dusk'}, 'y': 'Dusk'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch1= RunnablePassthrough()\n",
    "branch2= lambda z: z[\"input2\"]\n",
    "\n",
    "chain= RunnableParallel({\"x\": branch1, \"y\": branch2 })\n",
    "chain.invoke({\"input1\": \"Dawn\", \"input2\": \"Dusk\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9d27a7b-f631-4c08-9fe7-1a6c6009f659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'input1': 'heyy', 'input2': 'how are you?'}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new keys:\n",
    "def assign_func(_):\n",
    "    return \"new_branch_created\"\n",
    "\n",
    "chain= RunnableParallel({\"x1\": RunnablePassthrough()})\n",
    "chain.invoke({\"input1\": \"heyy\", \"input2\": \"how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f07c65b-7d44-4308-a7d2-bb5b9e543952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+  \n",
      "| Parallel<x1>Input |  \n",
      "+-------------------+  \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "    +-------------+    \n",
      "    | Passthrough |    \n",
      "    +-------------+    \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| Parallel<x1>Output | \n",
      "+--------------------+ \n"
     ]
    }
   ],
   "source": [
    "print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf501f2d-4d9f-4d29-92a1-50fa753011e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'input1': 'heyy', 'input2': 'how are you?'},\n",
       " 'x2': 'new_branch_created'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new keys:\n",
    "def assign_func(_):\n",
    "    return \"new_branch_created\"\n",
    "\n",
    "chain= RunnableParallel({\"x1\": RunnablePassthrough()}).assign(x2= RunnableLambda(assign_func))\n",
    "chain.invoke({\"input1\": \"heyy\", \"input2\": \"how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db02aaf6-db3f-44ad-96b6-22ab473b499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  +-------------------+                \n",
      "                  | Parallel<x1>Input |                \n",
      "                  +-------------------+                \n",
      "                            *                          \n",
      "                            *                          \n",
      "                            *                          \n",
      "                     +-------------+                   \n",
      "                     | Passthrough |                   \n",
      "                     +-------------+                   \n",
      "                            *                          \n",
      "                            *                          \n",
      "                            *                          \n",
      "                  +-------------------+                \n",
      "                  | Parallel<x2>Input |                \n",
      "                  +-------------------+                \n",
      "                   ***              ***                \n",
      "                ***                    ***             \n",
      "              **                          **           \n",
      "+---------------------+               +-------------+  \n",
      "| Lambda(assign_func) |               | Passthrough |  \n",
      "+---------------------+               +-------------+  \n",
      "                   ***              ***                \n",
      "                      ***        ***                   \n",
      "                         **    **                      \n",
      "                  +--------------------+               \n",
      "                  | Parallel<x2>Output |               \n",
      "                  +--------------------+               \n"
     ]
    }
   ],
   "source": [
    "print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d097200-3451-4d0c-a510-dcb114f56ef4",
   "metadata": {},
   "source": [
    "## 3. RAG with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b2bc65-5afb-4da0-ab4d-5eb3526215bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "os.environ[\"GOOGLE_API_KEY\"]= gemini_api_key\n",
    "llm= ChatGoogleGenerativeAI(model= \"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24502b2c-5e70-4b29-a03f-8ca0c8c10ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "embeddings= HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60140fb7-2f24-4697-9fa6-fcda8adc5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= [\n",
    "    Document(page_content= \"Most popular dog breed is Labrador.\", metadata= {\"source\":\"dogs.txt\"}),\n",
    "    Document(page_content= \"The favourite food for dogs is bone.\", metadata= {\"source\":\"dogs.txt\"}),\n",
    "    Document(page_content= \"Most popular cat breed is Domestic shorthair.\", metadata= {\"source\":\"cats.txt\"}),\n",
    "    Document(page_content= \"The favourite food for cats is milk.\", metadata= {\"source\":\"cats.txt\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "627baac0-b698-4be7-9169-60a9fc4d2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d718721-48b6-46bc-82c6-ade66ee5d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "69da455f-369a-490f-9184-2aa61f8c9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= '''Answer the following question based only the given context.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}'''\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b9a9370-167e-45ce-be9a-560e0b440b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bone \\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain= (\n",
    "    {\n",
    "        \"context\": (lambda x: x[\"question\"]) | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()    \n",
    ")\n",
    "retrieval_chain.invoke({\"question\": \"what is dogs' fav food?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d20ed-45d4-47ac-9336-5e84f35225f2",
   "metadata": {},
   "source": [
    "The first component is a dictionary, not a runnable. How is this working? <br>\n",
    "Langchain has a property called coersion. It automatically converts dictionary in to RunnableParallel<br><br>\n",
    "\n",
    "The 'x' comes from the .invoke()<br>\n",
    "\n",
    "_itemgetter(\"question\")_ is equivalent to _lambda x: x[\"question\"],_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2564c18e-ace3-4022-be76-e696975b7706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bone \\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a better way to write the chain:\n",
    "retrieval_chain= (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()    \n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"what is dogs' fav food?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf01b67-4436-4dd8-bc90-75a5eaeff346",
   "metadata": {},
   "source": [
    "## 4. Chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bb46e-a678-4a2a-abd1-c0beb0aad2a2",
   "metadata": {},
   "source": [
    "You ask a follow up question, for example: \"Explain in detail\". The retriever will take this and show top k docs, where all of them are completely irrelevant to given question. So we need a stand alone question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "06e33179-0e58-4abb-b26e-d31067193c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrasal_prompt_template= '''Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up question: {question}'''\n",
    "rephrase_template= PromptTemplate.from_template(rephrasal_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7d729f24-56cc-4d57-939b-69bbe62bc85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Is plum cake really a dog's favorite food? \\n\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrase_chain= rephrase_template | llm | StrOutputParser()\n",
    "\n",
    "prompt_rephrased= rephrase_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Are you sure?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content= \"what is dog's fav food?\"),\n",
    "            AIMessage(content= \"Plum cake\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "prompt_rephrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daec10-ea8f-4f6d-bfdc-8ab129d10524",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain= (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()},\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutpuParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "591db5c6-b16d-4ff0-826f-c586232f212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This document provides insufficient information to answer if a bone is truly a dog's favorite food. It only states that it is **\"The favourite food for dogs\"** according to the file \"dogs.txt\".  We need more context or evidence to determine if this statement is actually true. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain= rephrase_chain | retrieval_chain\n",
    "\n",
    "answer= full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Are you sure?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content= \"What is dog's fav food?\"),\n",
    "            AIMessage(content= \"Bone\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99def7e-f4fb-47ae-87e6-23cfca322e85",
   "metadata": {},
   "source": [
    "## 5. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ec8d2-1c32-4819-8fef-3ff9e5066c3a",
   "metadata": {},
   "source": [
    "- prevents duplicate entry uploads\n",
    "- avoids rewriting unchanged data\n",
    "- avoids recomputing embeddings of unchanged data\n",
    "\n",
    "clean up modes:<br>\n",
    "1. \"incremental\": if source doc files are deleted, their respective data in the db wont be deleted<br>\n",
    "2. \"full\": it will be deleted\n",
    "3. \"none\": no clean up, keeps the data inside db no matter what\n",
    "<br>\n",
    "\n",
    "NOTE: <br>\n",
    "- both \"incremental\" and \"full\" will delete older versions of mutated content, while \"none\" clean up mode wont.\n",
    "- All three prevent data redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f6ce1a-be43-4af6-9c63-2cd05166b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_STRING= f\"postgresql+psycopg2://{dbi.user}:{dbi.password}@{dbi.host}:{dbi.port}/vectordb-texts\"\n",
    "CONN_STRING= f\"dbname='vectordb-texts' user='{dbi.user}' host='{dbi.host}' password='{dbi.password}'\"\n",
    "COLLECTION_NAME= \"vectordb-texts\"\n",
    "TABLE_NAME= \"langchain_pg_embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e53e06-d3ca-4dfd-9532-d94204aef6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "doc_dir= \"C:\\\\Users\\\\HP\\Desktop\\\\chatbot\\\\texts\"\n",
    "persist_dir= os.path.join(doc_dir, \"new-text-datastore\")\n",
    "\n",
    "loader= DirectoryLoader('../', glob=\"**/texts/*.txt\")\n",
    "documents= loader.load() \n",
    "\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size= 500, chunk_overlap= 0)\n",
    "docs= text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e681a581-fdbd-4a4f-846b-d2a3227c5ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 0, 'num_updated': 0, 'num_skipped': 12, 'num_deleted': 0}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db= PGVector(\n",
    "    connection_string= CONNECTION_STRING,\n",
    "    collection_name= COLLECTION_NAME,\n",
    "    embedding_function= embeddings\n",
    ")\n",
    "namespace= f\"pgvector/{COLLECTION_NAME}\"\n",
    "record_manager= SQLRecordManager(namespace, db_url= CONNECTION_STRING)\n",
    "record_manager.create_schema()\n",
    "\n",
    "index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    db,\n",
    "    cleanup= \"full\",\n",
    "    source_id_key= \"source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099a781-dfc3-46ea-93b4-252222742e2d",
   "metadata": {},
   "source": [
    "## 6. RAGAS- Evaluating RAG's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b3fa3-438a-434e-b145-6ae98c315d25",
   "metadata": {},
   "source": [
    "It has 5 evaluation metrics:\n",
    "- Context Precision\n",
    "  - Evaluates whether all ground-truth relevant items in the contexts are ranked higher\n",
    "  - All relevant chunks should ideally be at the top ranks\n",
    "  - Higher scores indicating better precision\n",
    "- Context Recall\n",
    "  - Measures the extent to which the retrieved context aligns with the annotated answer\n",
    "  - Higher values indicating better performance\n",
    "- Context Relevancy\n",
    "  - Gauges the relevancy of the retrieved context based on both the question and contexts\n",
    "  - Higher values indicating better relevancy\n",
    "- Answer Relevancy\n",
    "  - Assesses how pertinent(relevant) the generated answer is to the given prompt\n",
    "  - Higher scores indicating better relevancy, determined by cosine similarity between the original and reverse engineered questions.\n",
    "- Faithfullness:\n",
    "  - Measures factual consistency of the generated answer against the given context\n",
    "  - Higher scores indicate better consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656ba55f-ec90-4fe9-8a46-65106586d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAGAS needs \"file_name\" metadata, so we create a new key\n",
    "for doc in docs:\n",
    "    doc.metadata[\"file_name\"]= doc.metadata[\"source\"]\n",
    "    del doc.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d774c-3665-4582-9782-7bce7814b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator= TestsetGenerator.from_langchain(\n",
    "    embeddings= embeddings,\n",
    "    generator_llm= llm,\n",
    "    critic_llm= llm\n",
    ")\n",
    "\n",
    "testset= generator.generate_with_langchain_docs(\n",
    "    docs,\n",
    "    test_size= 4,\n",
    "    distributions= {simple: 0.4, reasoning: 0.2, multi_context: 0.4}\n",
    ")\n",
    "\n",
    "Resource Exhausted 429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568d7d2-6c6b-4334-814d-2ecf5bf46c0e",
   "metadata": {},
   "source": [
    "## 7. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51bff1-1d33-4e00-b8e4-815e0c16d964",
   "metadata": {},
   "source": [
    "why chunking?\n",
    "- context window:\n",
    "  llms have limited context window (no of tokens they can process at a time)\n",
    "  becoming less important problem nowadays, (models nowadays come with higher context window)\n",
    "- Embeddings:\n",
    "  Chunking should be done in such a way that the semantics of the chunks are preserved well\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c452c2-fb73-49ba-9a7f-53f783e0cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 288, which is longer than the specified 200\n",
      "Created a chunk of size 261, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 277, which is longer than the specified 200\n",
      "Created a chunk of size 266, which is longer than the specified 200\n",
      "Created a chunk of size 228, which is longer than the specified 200\n",
      "Created a chunk of size 306, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 201, which is longer than the specified 200\n",
      "Created a chunk of size 347, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "with open(\"./texts/color-psychology.txt\") as f:\n",
    "    file= f.read()\n",
    "    \n",
    "text_splitter= CharacterTextSplitter(\n",
    "    separator= \"\\n\",\n",
    "    chunk_size= 200,\n",
    "    chunk_overlap= 20,\n",
    "    length_function= len,\n",
    "    is_separator_regex= False\n",
    ")\n",
    "\n",
    "docs= text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da1a56ec-38c8-4cf1-9178-41faf6a59678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f84c05-8ddf-4e60-a5a8-144e41fd929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 200,\n",
    "    chunk_overlap= 20,\n",
    "    length_function= len,\n",
    "    is_separator_regex= False\n",
    ")\n",
    "\n",
    "docs= text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e01b37e-fab4-4f75-8131-0873d238cbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b0a94-b98e-4e8a-81e7-0dcad07c1a57",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter is the recommended text splitter.<br>\n",
    "It still doesnt capture the semantic meaning well, <br>it just splits based on the specified characters, eg (\"\\n\\n\", \"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83357b-71bc-48cd-b115-20f9204eac78",
   "metadata": {},
   "source": [
    "A better approach is __Semantic Chunking__: (it is still experimental)<br>\n",
    "splits a text into individual sentences and then embeds the sentences,\n",
    "after embedding all the sentences, it compares and finds the similar chunks and creates a new chunk out of it.<br>\n",
    "<br>\n",
    "How many such embeddings should be aggregated into forming a chunk?<br>\n",
    "it has a predefined breakpoint threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28407e9b-89e4-4a7d-95f8-a88aa8faeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= SemanticChunker(embeddings, breakpoint_threshold_type= \"standard_deviation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "689a1c83-a337-4ac5-9790-e98e405d9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2bee5e-db54-4d52-86d4-7532a94379b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1a119-88c6-4499-a002-e51e76229c58",
   "metadata": {},
   "source": [
    "#### much better chunking technique: custom built with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d87f2d24-f18c-4cbe-8540-6ad898ab6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTSplitter(TextSplitter):\n",
    "    def __init__(self, model_name: str= \"gemini-1.5-pro-latest\", **kwargs: Any)-> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm= ChatGoogleGenerativeAI(model= model_name)\n",
    "        self.template= '''\n",
    "        You are an expert in identifying the semantic meaning of text.\n",
    "        You wrap each chunk in <<<>>>.\n",
    "        Example:\n",
    "        Text:\n",
    "        Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets. Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user. A person does have a system of thought patterns that will answer or draw a conclusion on something that he feels is new or interesting.\n",
    "\n",
    "        Wrapped:\n",
    "        <<<Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.>>>\n",
    "        <<<Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.>>>\n",
    "        <<<A person does have a system of thought patterns that will answer or draw a conclusion on something that he feels is new or interesting.>>>\n",
    "\n",
    "        Now, process the following text:\n",
    "        {text}\n",
    "        '''\n",
    "        self.prompt= ChatPromptTemplate.from_template(self.template)\n",
    "        self.output_parser= StrOutputParser()\n",
    "        self.chain= (\n",
    "            {\"text\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | self.output_parser\n",
    "        )\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        response= self.chain.invoke({\"text\": text})\n",
    "        chunks= re.findall(r\"<<<(.*?)>>>\", response, re.DOTALL)\n",
    "        return [chunk.strip() for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e65786aa-6c06-418c-8ac2-dd36060d644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_splitter= GPTSplitter()\n",
    "docs= gpt_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb729d9-4536-43f7-a67c-6139ddfc9cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5d22c31-de66-497f-b8b2-4215d7893e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying attention to visual concepts that are attractive to the eye.',\n",
       " 'That way, the colors used must also be appropriate and appropriate for the design created. We cannot possibly give a dominant black color to an image that shows a cheerful impression, nor can we give a bright dominant color such as blue, red, pink to an image that shows a sad impression.',\n",
       " 'Showing a picture with a sad impression, a painter will give a little boredom to the picture. It can be seen from the paintings depicting a sad event, most of them have a lot of color, but there is little saturation, it could be said that the saturation is low.',\n",
       " 'However, images that show the impression of love will usually have a lot of pink or other bright colors that show love.',\n",
       " 'Without realizing it, these things are one of the functions of the existence of theory which must also be studied by us, color psychology.',\n",
       " 'What is color psychology?',\n",
       " 'According to Wikipedia, color psychology is the study of color as a determinant of human behavior. Color has an invisible influence on human perception. Like when eating, we will tend to feel hot if the place where we eat is red, we will eat faster if the place where we eat is yellow.',\n",
       " 'This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.',\n",
       " 'If we target people who have an exclusive soul, then we can use colors that depict this, such as black, gold, gray or dark green. Of course, the combination of basic colors and complementary colors must be balanced, that’s why we have to study color theory.',\n",
       " 'Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.',\n",
       " 'Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.',\n",
       " 'How Color psychology works?',\n",
       " 'In short, a person does have a system of thought patterns that will answer or draw a conclusion on something that he feels is new or interesting.',\n",
       " 'Someone will see it, be interested in it, look for a conclusion from what they see, and finally react to what they feel. That’s what happens with color, someone will see the color, create a conclusion, and finally give a reaction, whether it’s bold, hot, or something else.',\n",
       " 'Colors and their meanings:',\n",
       " 'Each color certainly has its own meaning and significance, even white and black also have their own meaning. Not infrequently, some colors also have double meanings depending on what they are used for. So, what are the meanings of the colors that we often encounter?',\n",
       " '1. Black',\n",
       " 'Black is currently often used by most designers, whether creating designs for music or fitness groups. Not infrequently, people nowadays also prefer dark colors like black in everyday life.',\n",
       " 'So, what effect does black have on a person’s psychology? What does the color black mean? Black gives a formal and exclusive impression. But on the other hand, black can also be interpreted as the color of darkness or sadness.',\n",
       " '2. White',\n",
       " 'Next is white. If there is black which is a dark color, then there is white which is a bright color. White itself is often used at religious events, as well as as a base for other colors used. Sometimes, some designs also use white as a complement to the design or a balance between dark and bright colors.',\n",
       " 'The color white itself means clean, simple or holy. But on the other hand, white also has negative meanings, such as flat or boring.',\n",
       " '3. Red',\n",
       " 'Moving on from black and white, red is a quite unique color, because red has good and bad meanings. For example, people will consider red as a color that shows anger, chaos, etc. However, red can also be interpreted as a color that shows strength, enthusiasm or love.',\n",
       " '4. Purple',\n",
       " 'Next there is purple. Purple is usually identified with a color that gives the impression of enthusiasm and joy. The color purple is also used in designs that show warmth and concern for fellow humans.',\n",
       " 'Closure',\n",
       " 'You can follow our medium profile, to get information about graphic design, the world of design, technological developments, and various other things related to design. Let’s make this place a place for us to deepen our understanding of the world of design, and don’t forget to exchange ideas and information about design that we need to know.',\n",
       " 'I’m William, I hope this is useful for all of us, see you again.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f215d3a4-5afd-42eb-9fac-63ae6c419c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#improve the prompt; dont let the llm simply split according to new line character (in our case, it just did that)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158416fb-765f-490c-a079-983c130c6647",
   "metadata": {},
   "source": [
    "## 8. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d659037-fff1-4833-8a50-05923b7d491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector= embeddings.embed_documents(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e764fcff-5f59-407e-b1d3-ff8219aba009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa091c-3709-4a6d-ab54-686d5f9c29de",
   "metadata": {},
   "source": [
    "Embedding dimension count vary from model to model\n",
    "<br><br>\n",
    "HuggingFace embedding models have 384 dimensions<br>\n",
    "BERT= 768<br>\n",
    "OpenAI Embeddings= 1536, 3072<br>\n",
    "Cohere= 1024<br>\n",
    "Mistral= 1024<br>\n",
    "LLama 2= 5192<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202b06d-53c4-4114-bbfb-11b98c61b0cc",
   "metadata": {},
   "source": [
    "Higher embedding dimensions means higher accuracy, slower computation and more storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baaff3f-d341-4cfc-a829-b0ce88e12a76",
   "metadata": {},
   "source": [
    "## 9. Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb31baf-3b63-4cea-b04b-50b30ba87ca4",
   "metadata": {},
   "source": [
    "#### MultiQuery Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd43bfc-f6e2-49e5-9bbf-a8c47a6926a2",
   "metadata": {},
   "source": [
    "Generate different variations of the same input question. These questions may retrieve better content than the original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aba82b2c-06c6-41f5-b2d9-04bc5423e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"what color used by companies?\"\n",
    "\n",
    "QUERY_PROMPT= PromptTemplate(\n",
    "    input_variables= [\"question\"],\n",
    "    template= '''\n",
    "    You are an AI language model assistant, Your task is to generate five different versions\n",
    "    of the given user question to retrieve relevant documents from a vector database. By generating\n",
    "    multiple perspectives on the user question, your goal is to help the user overcome some of the\n",
    "    limitations of the distance-based similarity search.\n",
    "    Provide these alternative question like this:\n",
    "    <<question1>>\n",
    "    <<question2>>\n",
    "    Only provide the query, no numberings.\n",
    "    Original question: {question}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834c6b93-b8a1-47af-aec8-3d718f55e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(input):\n",
    "    return [item for item in re.split(r\"<<|>>\", input) if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28d8cad8-6086-4dbe-8012-6d830f477739",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_chain= QUERY_PROMPT | llm | StrOutputParser() | RunnableLambda(extract_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d15cf73-7789-4ea3-94d9-2f2f452a1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_queries= multiquery_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea306d88-47d4-49d7-a2fc-12af527467bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the most popular colors for company branding?',\n",
       " 'What is the psychology of color in branding and marketing?',\n",
       " 'How do companies choose brand colors?',\n",
       " 'What colors are associated with different industries?',\n",
       " 'What is the impact of color on consumer perception of brands?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0580f3-9cbd-4608-99ad-cbdf8c853af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "chunks= text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aaaf103-9870-4e3c-8879-4e20559e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on each inference\n",
    "db= Chroma.from_documents(chunks, embeddings)\n",
    "retriever= db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a43b258b-23d9-4731-9e71-97d55df452dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= []\n",
    "for query in list_of_queries:\n",
    "    data= retriever.invoke(query)\n",
    "    docs.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "692291aa-428c-476e-a87e-93ff51b02b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Colors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='What is color psychology? According to Wikipedia, color psychology is the study of color as a determinant of human behavior. Color has an invisible influence on human perception. Like when eating, we', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Colors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Colors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='colors must be balanced, that’s why we have to study color theory.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Colors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc3a8944-fba8-49fc-9228-84efabfd6fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71c116d2-28f5-4881-a238-fe480500d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_contents= set()\n",
    "final_docs= []\n",
    "for i in docs:\n",
    "    if i.page_content not in unique_contents:\n",
    "        final_docs.append(i)\n",
    "    unique_contents.add(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a099c13a-2015-4d9f-96ce-d46e95d8159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Colors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='What is color psychology? According to Wikipedia, color psychology is the study of color as a determinant of human behavior. Color has an invisible influence on human perception. Like when eating, we', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='colors must be balanced, that’s why we have to study color theory.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bb642-4e91-4bf7-aa55-efcdb27592ca",
   "metadata": {},
   "source": [
    "## 10. Parent Document Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad810505-ad3f-4072-a745-304a76df8f68",
   "metadata": {},
   "source": [
    "Smaller chunks provide better precision, but returns less context, <br>\n",
    "while the larger chunks retrieves more context, but they may not be precise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e60eea-b97f-4b22-994d-0d6544977685",
   "metadata": {},
   "source": [
    "Parent Document Retriever uses small chunks for similarity search, but returns its associated larger chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f92da-0555-4a9a-94ce-8481b382a90c",
   "metadata": {},
   "source": [
    "Langchain only provides InMemoryStore() for this, which is not suitable at larger scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3b42ae-ad10-4df2-a9a7-346bf578bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore= InMemoryStore() # stores larger chunks\n",
    "child_splitter= RecursiveCharacterTextSplitter(chunk_size= 250)\n",
    "parent_splitter= RecursiveCharacterTextSplitter(chunk_size= 600)\n",
    "\n",
    "retriever= ParentDocumentRetriever(\n",
    "    vectorstore= db,\n",
    "    docstore= docstore,\n",
    "    child_splitter= child_splitter,\n",
    "    parent_splitter= parent_splitter\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e4c6ed-6d39-4a30-8d1d-063a90ac52c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='How Color psychology works? In short, a person does have a system of thought patterns that will answer or draw a conclusion on something that he feels is new or interesting.\\n\\nSomeone will see it, be interested in it, look for a conclusion from what they see, and finally react to what they feel. That’s what happens with color, someone will see the color, create a conclusion, and finally give a reaction, whether it’s bold, hot, or something else.\\n\\nColors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"blue color\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb36939-5fb9-46a0-98e4-dc67b87c1eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(retriever.invoke(\"blue color\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38d0d6-4d33-4163-96d3-3942a967c36b",
   "metadata": {},
   "source": [
    "#### Custom docstore using PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed171317-f5f8-42ed-b442-0a6739b83c5e",
   "metadata": {},
   "source": [
    "doc store works with keys and values<br>\n",
    "key is referenced to \"doc_id\", or 'source' which is in the metadata dictonary of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ec3617-65fb-45e4-8784-676dfe645d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str]= Field(None)\n",
    "    page_content: Optional[str]= Field(None)\n",
    "    metadata: dict= Field(default_factory= dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca413224-cc63-4de6-b60b-837a996f3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base= declarative_base()\n",
    "'''\n",
    "table only consists of 2 columns: key and value\n",
    "why JSONB (JSON Binary), we cant store python classes in as entries in the table\n",
    "(the document datatype is Document class) \n",
    "So we serialize the data and store; when we retrieve, we de-serialize it \n",
    "binary data -> <class 'langchain_core.documents.base.Document'>\n",
    "'''\n",
    "class SQLDocument(Base):\n",
    "    __tablename__= \"docstore\"\n",
    "    key= Column(String, primary_key= True)\n",
    "    value= Column(JSONB) #serialization\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key= '{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4deddeff-7cc9-49cd-bb68-c5276514389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger= logging.getLogger(__name__)\n",
    "D= TypeVar(\"D\", bound= Document)\n",
    "\n",
    "#class inherits from langchain's base store (key: string, value: DocumentModel)\n",
    "class PostgresStore(BaseStore[str, DocumentModel], Generic[D]):\n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine= create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session= scoped_session(sessionmaker(bind= self.engine))\n",
    "\n",
    "    # the table has 2 rows, one for key and other one is value\n",
    "    def serialize_document(self, doc: Document) -> dict:\n",
    "        return {\"metadata\": doc.metadata, \"page_content\": doc.page_content}\n",
    "\n",
    "    def deserialize_document(self, value: dict) -> dict:\n",
    "        return Document(\n",
    "            metadata= value.get(\"metadata\", {}), # if we dont have, we set it to empty dict\n",
    "            page_content= value.get(\"page_content\", \"\")  # if we dont have, we set it to empty string          \n",
    "        )\n",
    "\n",
    "    def mget(self, keys: Sequence[str]) -> list[Document]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                sql_documents= (\n",
    "                    session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).all()\n",
    "                )\n",
    "                return [\n",
    "                    self.deserialize_document(sql_doc.value)\n",
    "                    for sql_doc in sql_documents\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mget: {e}\")\n",
    "                print(f\"Error in mget: {e}\")\n",
    "                session.rollback()\n",
    "                return []\n",
    "    def mset(self, key_value_pairs: Sequence[tuple[str, Document]]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                serialized_docs= []\n",
    "                for key, document in key_value_pairs:\n",
    "                    serialized_doc= self.serialize_document(document)\n",
    "                    serialized_docs.append((key, serialized_doc))\n",
    "                documents_to_update= [\n",
    "                    SQLDocument(key= key, value= value) for key, value in serialized_docs\n",
    "                ]\n",
    "                session.bulk_save_objects(documents_to_update, update_changed_only= True)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mset: {e}\")\n",
    "                print(f\"Error in mset: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    def mdelete(self, keys: Sequence[str])-> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).delete(\n",
    "                synchronize_session= False\n",
    "                )\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mdelete: {e}\")\n",
    "                print(f\"Error in mdelete: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    def yield_keys(self) -> Iterator[str]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                query= session.query(SQLDocument.key)\n",
    "                for key in query:\n",
    "                    yield key[0]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in yield _keys: {e}\")\n",
    "                print(f\"Error in yield _keys: {e}\")\n",
    "                session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7241bce1-1364-479f-94b3-39fc66d9f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\lang_chain_learn_ings\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:322: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "DATABASE_URL= CONNECTION_STRING\n",
    "store= PGVector(\n",
    "    collection_name= \"vectordb\",\n",
    "    connection_string= DATABASE_URL,\n",
    "    embedding_function= embeddings\n",
    ")\n",
    "child_splitter= RecursiveCharacterTextSplitter(chunk_size= 250)\n",
    "parent_splitter= RecursiveCharacterTextSplitter(chunk_size= 600)\n",
    "\n",
    "retriever= ParentDocumentRetriever(\n",
    "    vectorstore= store,\n",
    "    docstore= PostgresStore(connection_string= DATABASE_URL),\n",
    "    child_splitter= child_splitter,\n",
    "    parent_splitter= parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "629a0be9-ff33-45e8-8939-3142a854ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c239d9b3-f64b-4155-8c77-4cfd89d45050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.\\n\\nColor psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='How Color psychology works? In short, a person does have a system of thought patterns that will answer or draw a conclusion on something that he feels is new or interesting.\\n\\nSomeone will see it, be interested in it, look for a conclusion from what they see, and finally react to what they feel. That’s what happens with color, someone will see the color, create a conclusion, and finally give a reaction, whether it’s bold, hot, or something else.\\n\\nColors and their meanings:', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Next is white. If there is black which is a dark color, then there is white which is a bright color. White itself is often used at religious events, as well as as a base for other colors used. Sometimes, some designs also use white as a complement to the design or a balance between dark and bright colors.\\n\\nThe color white itself means clean, simple or holy. But on the other hand, white also has negative meanings, such as flat or boring.\\n\\n3. Red', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what color is used by companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc7d1f-64da-4063-a2c8-9da9566e851d",
   "metadata": {},
   "source": [
    "When to use Parent Document Retriever?<br>\n",
    "\n",
    "1. if you use an LLM with a large context window due to larger docs passes to the LLM<br>\n",
    "2. Useful if documents contain multiple topics, but splitting would loose overall meaning of these documents<br>\n",
    "_use and LLM based splitter in that case (captures the semantic meaning of chunk and easier to implement_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270f340-9e14-4dbf-a8a6-0ec09d3e2205",
   "metadata": {},
   "source": [
    "## 11. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bcbd3a-395b-40d1-bdc5-24122c8372a0",
   "metadata": {},
   "source": [
    "- Entity that can make decisions and take actions to achieve specific goals\n",
    "- LLM acts as a \"brain\" for these agents\n",
    "- Agents can utilize tools which allows them to communicate with the outside world (internet search, call API, perform RAG etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ba38354-d7a0-4500-a705-b7b79a515930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebb4e0ac-c300-45ee-a638-8f342727dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= Chroma.from_documents(docs, embeddings)\n",
    "retriever= db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59d2823e-572e-4ed0-a1a1-f83993d6f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool= create_retriever_tool(\n",
    "    retriever= retriever, \n",
    "    name= \"ragagent\",\n",
    "    description= \"performs RAG on a small dataset\"\n",
    ")\n",
    "\n",
    "tools= [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa0cd54b-9f5a-4c04-b2dc-e22d3869ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatGoogleGenerativeAI(model= \"gemini-1.5-pro\", convert_system_message_to_human= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd52ceb6-823e-49d0-a59e-b24e2b600b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor= AgentExecutor(agent= agent, tools= tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05dd28-0256-424e-bb86-d8876f5766ab",
   "metadata": {},
   "source": [
    "for an agent, we need an AgentExecutor (it is the runtime for that agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a59d72c-a0f3-4ab8-a83f-d32908e31ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= agent_executor.invoke({\"input\": \"which color is said to represent companies?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05e85a90-39cc-41bc-96c3-f1f6ea2bae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The color most commonly associated with companies and corporations is **blue**. \n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Trustworthiness and Reliability:** Blue conveys a sense of security, stability, and dependability – qualities important for businesses to project.\n",
       "* **Professionalism and Competence:**  It's seen as a formal and authoritative color, often used in business suits and corporate settings.\n",
       "* **Calmness and Communication:** Blue can have a calming effect and is associated with clear communication, making it suitable for building relationships.\n",
       "\n",
       "While blue is a dominant choice, other colors are also used strategically by companies depending on their industry and brand identity. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b772d-d055-44c0-b103-c44c5ecfb932",
   "metadata": {},
   "source": [
    "## 12. Re-Ranking retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf1656-ab6a-44dd-a9c3-5789155a8766",
   "metadata": {},
   "source": [
    "use a cross encoder to compare how similar the retrieved documents are compared to the query<br>\n",
    "but we have the embeddings right?<br><br>\n",
    "The embeddings we used, uses Bi- Encoder, and here we use Cross- Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebd9f7-3430-4cbe-ac24-cc29f867f311",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fae906-5e09-4791-8efc-260ac239060a",
   "metadata": {},
   "source": [
    "Bi Encoder: \n",
    "- Generate independent sentence embeddings \n",
    "- then we can project these embeddings in a N-d space and see similarity\n",
    "- use cases: info retrieval, semantic search, clustering\n",
    "\n",
    "Cross Encoder: \n",
    "- Encodes 2 sentences together and produces an output score\n",
    "- relatively more accurate, slow and less scalable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63759680-c8a1-4af4-9584-7e9b219b22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs= retriever.invoke(\"which color does companies use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954b4694-1ce7-4121-a360-511fb0a6fea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.\\n\\nColor psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.\\n\\nIf we target people who have an exclusive soul, then we can use colors that depict this, such as black, gold, gray or dark green. Of course, the combination of basic colors and complementary colors must be balanced, that’s why we have to study color theory.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='That way, the colors used must also be appropriate and appropriate for the design created. We cannot possibly give a dominant black color to an image that shows a cheerful impression, nor can we give a bright dominant color such as blue, red, pink to an image that shows a sad impression.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}),\n",
       " Document(page_content='Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying attention to visual concepts that are attractive to the eye.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8c901b-963b-4710-894a-d01c2d56be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder= CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "contents= [doc.page_content for doc in retrieved_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b011a80d-818e-4182-b76f-65cc90020748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8563137 , -0.75353575, -0.43809405, -7.37597   ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs= []\n",
    "for text in contents:\n",
    "    pairs.append([\"which color does companies use?\", text])\n",
    "\n",
    "scores= cross_encoder.predict(pairs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798474ad-2368-46c0-a3a8-b7faec89e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.43809405,\n",
       "  Document(page_content='That way, the colors used must also be appropriate and appropriate for the design created. We cannot possibly give a dominant black color to an image that shows a cheerful impression, nor can we give a bright dominant color such as blue, red, pink to an image that shows a sad impression.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})),\n",
       " (-0.75353575,\n",
       "  Document(page_content='This proves how influential a color is on a person’s perception of something. Of course, knowing color psychology is a must for a designer to create a design that suits the target.\\n\\nIf we target people who have an exclusive soul, then we can use colors that depict this, such as black, gold, gray or dark green. Of course, the combination of basic colors and complementary colors must be balanced, that’s why we have to study color theory.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})),\n",
       " (-1.8563137,\n",
       "  Document(page_content='Color psychology is also often used for marketing purposes. In the world of marketing, of course color psychology is needed so that the design can be conveyed well by the specified marketing targets.\\n\\nColor psychology is also very useful for fashion trends. The colors used and the combination of colors must of course suit the user.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'})),\n",
       " (-7.37597,\n",
       "  Document(page_content='Color Psychology — as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying attention to visual concepts that are attractive to the eye.', metadata={'source': '..\\\\chatbot\\\\texts\\\\color-psychology.txt'}))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_docs= zip(scores, retrieved_docs)\n",
    "sorted_docs= sorted(scored_docs, reverse= True)\n",
    "sorted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160864c-08f2-4858-92ad-bb4ac1ff09a6",
   "metadata": {},
   "source": [
    "Draw backs of cross-encoder:\n",
    "- scores are relative\n",
    "- sometimes we need more than 1 chunk to answer a question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a466c-dc42-474f-9bcb-efbd92da922a",
   "metadata": {},
   "source": [
    "#### LLM Based Document Compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877d8dfb-520c-42bf-9909-b7fc9648bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_EVALUATION_PROMPT= PromptTemplate(\n",
    "    input_variables= [\"document\", \"question\"],\n",
    "    template= '''\n",
    "    You are an AI language model assistant. Your task is to evaluate the provided document to determine \n",
    "    if it is suited to answer the given user question. Assess the document for its relevance to the question, \n",
    "    the completeness of information, and the accuracy of the content.\n",
    "\n",
    "    Original question: {question}\n",
    "    Document for Evaluation: {document}\n",
    "    Evaluation Result: <<'True' if the document is suited to answer the question, 'False' if it is not>>\n",
    "\n",
    "    Note: Conclude with a 'True' or 'False' based on your analysis of the document's relevance,\n",
    "    completeness, and accuracy in relation to the question.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86d758-52c7-43d5-88ed-9d5127f9795c",
   "metadata": {},
   "source": [
    "use this prompt on the question and answer pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d6357-20e9-40f4-a4e7-2511eb78e2ed",
   "metadata": {},
   "source": [
    "## 13. Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff76487-f7e0-4415-86f1-d6fe8c5596e6",
   "metadata": {},
   "source": [
    "LLms are language models, they struggle to work with tabular or structured data<br>\n",
    "when you pass a table to llm, it looks like one large string<br>\n",
    "<br>\n",
    "How to perform RAG then?\n",
    "- Store data in SQL Database and let LLM write queries\n",
    "- Tool Calling: Let an LLM interact via custom functions (APIs work great for this)\n",
    "\n",
    "But what if we want to use both in one app?<br>\n",
    "Routing allows to use one **or** the other based on the input (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08c93ee1-f8b0-433e-962f-9fd7e38fadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_expert_template= '''\n",
    "You are a Fruit expert. You have broad knowledge about fruits, their benefits, their characteristics etc..\n",
    "You proved clear answer to questions about fruits.\n",
    "\n",
    "Here is a question: \n",
    "{question}'''\n",
    "\n",
    "florist_template= '''\n",
    "You are a florist. You have broad knowledge about flowers, plants and gardening.\n",
    "You proved clear answer to questions about those.\n",
    "\n",
    "Here is a question: \n",
    "{question}'''\n",
    "\n",
    "medicine_expert_template= '''\n",
    "You are an expert physician. You have broad knowledge about medicine, health issues,  etc..\n",
    "You proved clear answer to questions about health issues. DO NOT deflect or evade any type of questions\n",
    "as it may help the audience gain good awarness about their condition. You can always mention to the user to contact\n",
    "an actual doctor at the end of your response.\n",
    "\n",
    "Here is a question: \n",
    "{question}'''\n",
    "\n",
    "\n",
    "fruits_questions= [\n",
    "    \"which tastes more bitter? apples, cherries or plums?\",\n",
    "    \"which fruits have only one seed?\",\n",
    "    \"is tomato a fruit or a vegetable?\"\n",
    "]\n",
    "\n",
    "health_questions= [\n",
    "    \"how and why hiccups occur?\",\n",
    "    \"what is visual snow?\",\n",
    "    \"what causes upper stomach pain?\"\n",
    "]\n",
    "\n",
    "flowers_questions= [\n",
    "    \"why do sunflowers face the sun?\",\n",
    "    \"how long does hibiscus take to grow?\",\n",
    "    \"which flowers are best suited for black soil?\"\n",
    "]\n",
    "\n",
    "fruits_embeddings= embeddings.embed_documents(fruits_questions)\n",
    "health_embeddings= embeddings.embed_documents(health_questions)\n",
    "flowers_embeddings= embeddings.embed_documents(flowers_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c493e7-d17a-4586-b753-0937cbe9a7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35898804, 0.5385415 , 0.49159018]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question= \"Mango is the king of fruits\"\n",
    "temp= embeddings.embed_query(sample_question)\n",
    "cosine_similarity([temp], fruits_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "333c7770-29f5-4ec6-8b0c-9add347c6a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35898804, 0.5385415 , 0.49159018])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([temp], fruits_embeddings)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd8e7f42-e4af-4712-8647-f1b6d4a093bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_router(input):\n",
    "    query_embedding= embeddings.embed_query(input[\"question\"])\n",
    "    fruits_similarity= cosine_similarity([query_embedding], fruits_embeddings)[0]\n",
    "    health_similarity= cosine_similarity([query_embedding], health_embeddings)[0]\n",
    "    flowers_similarity= cosine_similarity([query_embedding], flowers_embeddings)[0]\n",
    "\n",
    "    max_similarity= max(max(fruits_similarity), max(health_similarity), max(flowers_similarity))\n",
    "    if max_similarity== max(fruits_similarity):\n",
    "        print(\"Calling Fruit Expert\\n\")\n",
    "        return PromptTemplate.from_template(fruits_expert_template)\n",
    "    elif max_similarity== max(health_similarity):\n",
    "        print(\"Calling Physician\\n\")\n",
    "        return PromptTemplate.from_template(medicine_expert_template)\n",
    "    else:\n",
    "        print(\"Calling Flourist\\n\")\n",
    "        return PromptTemplate.from_template(florist_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f36a2a16-eb87-49e8-88e2-36a00881ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Physician\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question= \"List down some very painful medical conditions a human can experience\"\n",
    "input_query= {\"question\": question}\n",
    "prompt= prompt_router(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b19d1f9-9abe-4549-b7c5-2bb1c19db654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Physician\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It's important to preface this list by acknowledging that pain is subjective and individual. What one person experiences as excruciating, another may find manageable. This list is also not exhaustive and does not substitute for professional medical advice. \n",
       "\n",
       "That being said, here are some medical conditions known to cause extreme pain:\n",
       "\n",
       "**Acute Conditions:**\n",
       "\n",
       "* **Kidney Stones:** The passing of a kidney stone can cause sudden, sharp, and intense pain in the back, side, or groin.\n",
       "* **Cluster Headaches:** Often described as the \"most painful headache,\" cluster headaches cause severe, debilitating pain on one side of the head, typically around the eye.\n",
       "* **Gout:** This inflammatory arthritis causes sudden, severe attacks of pain, swelling, and tenderness in joints, often the big toe.\n",
       "* **Acute Pancreatitis:** Inflammation of the pancreas can lead to sudden, agonizing pain in the upper abdomen that radiates to the back.\n",
       "* **Shingles:** This viral infection causes a painful rash and burning sensation along a nerve pathway.\n",
       "\n",
       "**Chronic Conditions:**\n",
       "\n",
       "* **Trigeminal Neuralgia:** This neurological disorder causes excruciating facial pain, often triggered by simple acts like touching the face or chewing.\n",
       "* **Complex Regional Pain Syndrome (CRPS):** A chronic pain condition that typically affects one limb after an injury or surgery, causing severe burning, swelling, and sensitivity to touch. \n",
       "* **Fibromyalgia:** Characterized by widespread musculoskeletal pain, fatigue, and other symptoms like sleep problems and mood disorders.\n",
       "* **Sickle Cell Crisis:** In sickle cell disease, abnormally shaped red blood cells can block blood flow, causing severe pain episodes in various parts of the body.\n",
       "* **Endometriosis:** This condition occurs when tissue similar to the uterine lining grows outside the uterus, often leading to debilitating pelvic pain, especially during menstruation. \n",
       "\n",
       "**Other:**\n",
       "\n",
       "* **Burns:** Severe burns cause immense pain due to damage to nerve endings and tissues.\n",
       "* **Bone Fractures:** Depending on the severity and location, bone fractures can be incredibly painful.\n",
       "* **Cancer Pain:** Pain associated with cancer can be caused by the tumor itself, treatments like chemotherapy, or complications.\n",
       "\n",
       "This list provides a glimpse into some incredibly painful medical conditions. It's crucial to remember that everyone experiences pain differently, and seeking medical help is essential for diagnosis, pain management, and treatment.\n",
       "\n",
       "**It is vital to consult a healthcare professional for any health concerns or before making any decisions related to your health or treatment.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response= chain.invoke(question)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c95db-53f0-4748-a75a-fc603ee7c124",
   "metadata": {},
   "source": [
    "<br><br>if there are edge cases, the embeddings may not work.\n",
    "Use and llm to classify the type of question. <br>\n",
    "<br>prompt= '''you are good at classifiying a question. \n",
    "Given the user question below, classify it as either being about \"Health\", \"Flowers\" or \"Fruits\".\n",
    "<If the question is about flowers, plants and gardening, return \"Flowers\"><br>\n",
    "<If the question is about health issues, pain and medicine, return \"Health\"><br>\n",
    "<If the question is about fruits, natural food and eatery, return \"Fruits\"><br>\n",
    "\n",
    "Question:<br>\n",
    "{question}\n",
    "\n",
    "Classification: '''\n",
    "<br>\n",
    "now if Classification == \"Health\": call Physician etc..\n",
    "\n",
    "**NOTE:** Always do classification using LLM over embeddings\n",
    "- it is cheap\n",
    "- it is accurate\n",
    "- easy to maintain code/promptt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ba211-228b-43c7-ad8f-9679cd314cb9",
   "metadata": {},
   "source": [
    "Routing with vectorstore(text data) and database(tabular data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8187058-834a-43d5-a39e-e2f4f7a2874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL= CONNECTION_STRING\n",
    "store= PGVector(\n",
    "    collection_name= \"vectordb\",\n",
    "    connection_string= DATABASE_URL,\n",
    "    embedding_function= embeddings\n",
    ")\n",
    "\n",
    "loader1= TextLoader(doc_dir + \"/color-psychology.txt\")\n",
    "loader2= TextLoader(doc_dir + \"/natural-disasters.txt\")\n",
    "\n",
    "docs2= loader2.load()\n",
    "docs1= loader1.load()\n",
    "docs= docs1+ docs2\n",
    "\n",
    "splitter= RecursiveCharacterTextSplitter(chunk_size= 250, chunk_overlap= 20)\n",
    "chunks= splitter.split_documents(docs)\n",
    "store.add_documents(chunks)\n",
    "retriever= store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7afbdbdb-2917-4459-86df-abccacb39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "template= '''\n",
    "Based on the table schema below, write an SQL query (just the Query) that would answer the user's query.\n",
    "Do not add comments, Do not do any corrections:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: '''\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f8487de-d3c3-467d-91a2-cfb10d76a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= SQLDatabase.from_uri(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c59d6fe-d364-4016-9316-fdfb6865e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    engine= create_engine(CONNECTION_STRING)\n",
    "    inspector= inspect(engine)\n",
    "    columns= inspector.get_columns(\"fruits_and_vegetables\")\n",
    "\n",
    "    column_data= [\n",
    "        {\n",
    "            \"Column Name\": col[\"name\"],\n",
    "            \"Data Type\": str(col[\"type\"]),\n",
    "            \"Nullable\": \"Yes\" if col[\"nullable\"] else \"No\",\n",
    "            \"Default\": col[\"default\"] if col[\"default\"] else \"None\",\n",
    "            \"Autoincrement\": \"Yes\" if col[\"autoincrement\"] else \"No\",\n",
    "        }\n",
    "        for col in columns        \n",
    "    ]\n",
    "    schema_output= tabulate(column_data, headers= \"keys\", tablefmt= \"grid\")\n",
    "    formatted_schema= f\"Schema for 'fruits_and_vegetables' table:\\n{schema_output}\"\n",
    "    return formatted_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b42c825-89c5-4e10-a9ad-2587cdf60e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn= psycopg2.connect(CONN_STRING)\n",
    "cursor= conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "066eeee2-8eb6-4089-b455-9ddf6443dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query= '''\n",
    "CREATE TABLE IF NOT EXISTS fruits_and_vegetables (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name VARCHAR(20) UNIQUE,\n",
    "    price DECIMAL(10,2),\n",
    "    color TEXT,\n",
    "    category TEXT\n",
    ");\n",
    "'''\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "with open(doc_dir + \"/fruits-vegetables.txt\", \"r\") as file:\n",
    "    food_items= file.readlines()\n",
    "\n",
    "for line in food_items:\n",
    "    name, price, color, category= line.strip().split(\", \")\n",
    "    price= price[3:]\n",
    "    insert_query= '''\n",
    "    INSERT INTO fruits_and_vegetables (name, price, color, category)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    ON CONFLICT (name) DO NOTHING;\n",
    "    '''\n",
    "    cursor.execute(insert_query, (name, price, color, category))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7deaa4d-17be-4b10-bbec-867b84221b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Apple', Decimal('105.00'), 'green', 'fruit')\n",
      "(2, 'Apricot', Decimal('40.00'), 'orange', 'fruit')\n",
      "(3, 'Watermelon', Decimal('125.00'), 'green', 'fruit')\n",
      "(4, 'Banana', Decimal('30.00'), 'yellow', 'fruit')\n",
      "(5, 'Carrot', Decimal('28.00'), 'yellow', 'vegetable')\n",
      "(6, 'Mango', Decimal('140.00'), 'pink', 'fruit')\n",
      "(7, 'Beetroot', Decimal('110.00'), 'red', 'fruit')\n",
      "(8, 'Cherry', Decimal('20.00'), 'red', 'fruit')\n",
      "(9, 'Custard Apple', Decimal('100.00'), 'red', 'fruit')\n",
      "(10, 'Orange', Decimal('45.00'), 'orange', 'fruit')\n",
      "(11, 'Potato', Decimal('32.00'), 'purple', 'vegetable')\n",
      "(12, 'Spinach', Decimal('120.00'), 'red', 'fruit')\n",
      "(13, 'Plum', Decimal('115.00'), 'yellow', 'fruit')\n",
      "(14, 'Kiwi', Decimal('35.00'), 'green', 'fruit')\n",
      "(15, 'Grapes', Decimal('120.00'), 'purple', 'fruit')\n",
      "(16, 'Pomogranete', Decimal('25.00'), 'orange', 'vegetable')\n",
      "(17, 'Cabbage', Decimal('130.00'), 'green', 'fruit')\n",
      "(18, 'Pineapple', Decimal('50.00'), 'orange', 'fruit')\n",
      "(19, 'Cauliflower', Decimal('90.00'), 'red', 'fruit')\n",
      "(20, 'Pear', Decimal('22.00'), 'red', 'fruit')\n"
     ]
    }
   ],
   "source": [
    "conn= psycopg2.connect(CONN_STRING)\n",
    "cursor= conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM fruits_and_vegetables;\")\n",
    "products= cursor.fetchall()\n",
    "for product in products:\n",
    "    print(product)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cd8dd67-51fb-475b-b24e-3c11b6b115aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for 'fruits_and_vegetables' table:\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n",
      "| Column Name   | Data Type      | Nullable   | Default                                           | Autoincrement   |\n",
      "+===============+================+============+===================================================+=================+\n",
      "| id            | INTEGER        | No         | nextval('fruits_and_vegetables_id_seq'::regclass) | Yes             |\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n",
      "| name          | VARCHAR(20)    | Yes        | None                                              | No              |\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n",
      "| price         | NUMERIC(10, 2) | Yes        | None                                              | No              |\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n",
      "| color         | TEXT           | Yes        | None                                              | No              |\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n",
      "| category      | TEXT           | Yes        | None                                              | No              |\n",
      "+---------------+----------------+------------+---------------------------------------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4234be9-69ef-4add-8ae1-e907cd151288",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_response= (\n",
    "    RunnablePassthrough.assign(schema= get_schema)\n",
    "    | prompt\n",
    "    | llm.bind(stop= [\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sql_query= sql_response.invoke({\"question\": \"What is the color of price, color and category of 'pomogranete'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "21776b76-2386-4560-9860-5716ac7cfd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"```sql\\nSELECT price, color, category FROM fruits_and_vegetables WHERE name = 'pomogranete'\\n```\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "32584135-c85a-4914-98f9-d7b1192c0adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SELECT price, color, category FROM fruits_and_vegetables WHERE name = 'pomogranete'\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query= sql_query[7:-3]\n",
    "Markdown(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edc0a0a7-2917-4cc7-b072-5157c869a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "template= '''Based on the table schema below, question, sql query, and sql responses, \n",
    "write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}'''\n",
    "\n",
    "prompt_response= ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "    #query= query[7:-3]\n",
    "    query= query.replace(\"`\", \"\")\n",
    "    return db.run(query)\n",
    "    \n",
    "def debug(input):\n",
    "    print(\"SQL Output: \", input[\"query\"])\n",
    "    return input\n",
    "\n",
    "sql_chain= (\n",
    "    RunnablePassthrough.assign(query= sql_response).assign(\n",
    "        schema= get_schema,\n",
    "        response= lambda x: run_query(x[\"query\"]),\n",
    "    )\n",
    "    | RunnableLambda(debug)\n",
    "    | prompt_response\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "09848e8e-88b1-427a-abc0-4c402358d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Output:  ```sql\n",
      "SELECT price, color, category FROM fruits_and_vegetables WHERE name = 'Potato'\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "final_result= sql_chain.invoke({\"question\": \"What is the color of price, color and category of 'Potato'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b695225-ec61-43c6-8b3b-90d4430676af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Potato costs $32.00, is purple in color, and is categorized as a vegetable. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47df686-dee2-4c7c-a9c3-3c5dd2a6f9c6",
   "metadata": {},
   "source": [
    "but this is not safe...users could ask to \"delete all files\"..and llm will formulate a delete sql query operation, and it will be used by the cursor to delete the information in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22997de5-297d-4cff-a46e-623af31454a1",
   "metadata": {},
   "source": [
    "#### Prevent SQL Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c42ad-89e3-4462-b903-94a188f6b5d2",
   "metadata": {},
   "source": [
    "You can let the llm detect malicious user inputs by changing the prompts:<br>\n",
    "\n",
    "prompt='''<br>Based on the table schema below, write an SQL query (just the Query) that would answer the user's query.<br>\n",
    "If the query appears to be modifying or deleting database contents, return this query: SELECT \"Haha nice try! Got ya\"<br>'''\n",
    "\n",
    "This will provide some level of security, but it is not enough. <br>We have to make sure that the LLM doesn't have write access, only read access\n",
    "\n",
    "**Solution:** Create a user with read only access and provide the user details in connection string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5e6c0d-d912-451d-ba6e-3f0e9122754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read only user read_only_user created successfully\n"
     ]
    }
   ],
   "source": [
    "conn= psycopg2.connect(CONN_STRING)\n",
    "cursor= conn.cursor()\n",
    "user_name= \"read_only_user\"\n",
    "pass_word= \"read_only_pass\"\n",
    "try:\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"CREATE USER {} WITH  PASSWORD %s\"\n",
    "        ).format(\n",
    "            sql.Identifier(user_name)\n",
    "        ),\n",
    "        [pass_word]\n",
    "    )\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"GRANT CONNECT ON DATABASE {} TO {}\"\n",
    "        ).format(\n",
    "            sql.Identifier(conn.info.dbname),\n",
    "            sql.Identifier(user_name)\n",
    "        )\n",
    "    )\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"GRANT SELECT ON ALL TABLES IN SCHEMA public TO {}\"\n",
    "        ).format(\n",
    "            sql.Identifier(user_name)\n",
    "        )\n",
    "    )\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO {}\"\n",
    "        ).format(\n",
    "            sql.Identifier(user_name)\n",
    "        )\n",
    "    )\n",
    "    conn.commit()\n",
    "    print(f\"Read only user {user_name} created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"Error creating new user '{user_name}': {e}\")\n",
    "finally:\n",
    "    if cursor is not None:\n",
    "        cursor.close()\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69a0d93-6de7-453c-9d8f-96a760882e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn= psycopg2.connect(CONN_STRING)\n",
    "cursor= conn.cursor()\n",
    "try:\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"SELECT usename FROM pg_user\"\n",
    "        )\n",
    "    )\n",
    "    users= cursor.fetchall()\n",
    "    cursor.execute(\n",
    "        sql.SQL(\n",
    "            \"SELECT rolname AS role_name, rolsuper AS is_superuser FROM pg_roles\"\n",
    "        )\n",
    "    )\n",
    "    roles= cursor.fetchall()\n",
    "finally:\n",
    "    if cursor is not None:\n",
    "        cursor.close()\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22327983-c38c-4c81-bb0e-c9e8124055c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('postgres',), ('read_only_user',)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c36374-c59f-4bf4-82cd-6eb9c50a69bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pg_database_owner', False),\n",
       " ('pg_read_all_data', False),\n",
       " ('pg_write_all_data', False),\n",
       " ('pg_monitor', False),\n",
       " ('pg_read_all_settings', False),\n",
       " ('pg_read_all_stats', False),\n",
       " ('pg_stat_scan_tables', False),\n",
       " ('pg_read_server_files', False),\n",
       " ('pg_write_server_files', False),\n",
       " ('pg_execute_server_program', False),\n",
       " ('pg_signal_backend', False),\n",
       " ('pg_checkpoint', False),\n",
       " ('pg_use_reserved_connections', False),\n",
       " ('pg_create_subscription', False),\n",
       " ('postgres', True),\n",
       " ('read_only_user', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730ec465-0bf7-4113-b88c-a51b436e082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\lang_chain_learn_ings\\Lib\\site-packages\\langchain_community\\utilities\\sql_database.py:122: SAWarning: Did not recognize type 'vector' of column 'embedding'\n",
      "  self._metadata.reflect(\n"
     ]
    }
   ],
   "source": [
    "read_only_user_connection_string= f\"postgresql+psycopg2://{user_name}:{pass_word}@{dbi.host}:{dbi.port}/vectordb-texts\"\n",
    "db= SQLDatabase.from_uri(read_only_user_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f48313f-be10-45e7-9e4b-bc6068f0e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANNOT DO THAT\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result= sql_chain.invoke({\"question\": \"Drop all data from fruits_and_vegetables table\"})\n",
    "except ProgrammingError as pe:\n",
    "    if isinstance(pe.orig, InsufficientPrivilege):\n",
    "        result= \"CANNOT DO THAT\"\n",
    "    else:\n",
    "        result= f\"Unexpected error occured: {pe}\"\n",
    "except Exception as e:\n",
    "    result= f\"An unexpected error occured: {e}\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef826194-1267-4c2c-aebe-e7e8b0920e37",
   "metadata": {},
   "source": [
    "#### Routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc1d3434-9b0e-4a23-9bfb-c31f4cdbb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template= PromptTemplate.from_template(\n",
    "    \"\"\"You are good at classifying a question.\n",
    "    Given the user question below, classify it as either being about `Database`, `Chat` or 'Offtopic'.\n",
    "\n",
    "    <If the question is about fruits, flowers or health issues, classify the question as 'Database'>\n",
    "    <If the question is about natural disasters or color psychology or related topics, classify it as 'Chat'>\n",
    "    <If the question is about weather, football or anything not related to the above topics, classify it as 'offtopic'>\n",
    "    No need any comments. Just the category.\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Classification:\"\"\"\n",
    ")\n",
    "\n",
    "classification_chain= classification_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba52a5c6-82e5-430d-81c1-edbc7147878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offtopic \\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_chain.invoke({\"question\": \"does sun really rise in east and set in west?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f9a34e3b-176a-4e87-b40c-3b6f7f3662d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template= '''Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": (lambda x: x[\"question\"]) | retriever,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "737f6eb6-dc29-491d-8276-18fb82d15a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"database\" in info[\"topic\"].lower():\n",
    "        return sql_chain\n",
    "    elif \"chat\" in info[\"topic\"].lower():\n",
    "        return rag_chain\n",
    "    else:\n",
    "        \"I am sorry, I am not allowed to answer about the topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dd701636-07fd-4d2a-b20b-fd29013cd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain= RunnableParallel(\n",
    "    {\n",
    "        \"topic\": classification_chain,\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    ")| RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb80f084-715c-426b-8562-8e932fae6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Output:  ```sql\n",
      "SELECT name FROM fruits_and_vegetables WHERE category = 'fruit' ORDER BY price DESC LIMIT 1;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response= full_chain.invoke({\"question\": \"Whats the most expensive fruit?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f7783c5b-4ba3-4e9e-b667-90095110b6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The most expensive fruit is Mango. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "31732817-8feb-4008-9401-cf25de1780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= full_chain.invoke({\"question\": \"Which color represents companies?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cce67f00-cc9f-4107-8560-4e5e7007d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I cannot answer this question based on the context provided. The given text snippets mention \"colors and their meanings\" and \"color theory,\" but they do not specify which color represents companies. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ca14cf7-6112-4424-8263-5fd9fe12d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"Who will win? Master Oogway or Dragon Warrior Po?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3e462-69c3-47c9-9a9f-c967bc54dd1c",
   "metadata": {},
   "source": [
    "## 14. NeMo- Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a161d3-f07b-4202-ba3b-ff8efdc95eb5",
   "metadata": {},
   "source": [
    "- Open-source toolkit for easily adding programmable guardrails to LLM based conversational applications\n",
    "- Guardrails are specific ways of controlling the output of LLM\n",
    "\n",
    "Benefits:\n",
    "- Building a safe, secure and trustworthy llm based applications easily\n",
    "- Prevent LLM from talking about specific topics (politics etc..)\n",
    "- Allows to easily design best practices (e.g. Authentication)\n",
    "- Controllable dialog: you can steer the LLM to follow pre-defined conversational paths (ordering process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3e1e2f-866a-4a85-b2ea-d4a0c80853b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply() #to make nemo guardrails work in a jupyter environment\n",
    "\n",
    "colang_content= '''\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you today?\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "'''\n",
    "\n",
    "yaml_content= '''\n",
    "models:\n",
    "- type: main\n",
    "  engine: vertexai\n",
    "  model: gemini-1.5-pro-latest\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31efd6-a749-44b4-bf3c-a87d04f5f8b2",
   "metadata": {},
   "source": [
    "colang is a modelling language for conversational applications. we use colang to design how the conversation between the user and the bot should happen. in colang the two core concepts are messages and flows. In colang, conversation is modelled as an exchange of a user message and bot message.It is like trying to teach an LLM different ways of saying hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bba5cde-8a1d-4d81-9eb5-0a284c6ad2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config= RailsConfig.from_content(\n",
    "    yaml_content= yaml_content,\n",
    "    colang_content= colang_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be050c-3026-4db2-9b0f-6556b192d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implemented for ChatGoogleGenerativeAI\n",
    "rails= LLMRails(config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f0ecf-4d97-4b4a-8c6e-4a9e51c6283d",
   "metadata": {},
   "source": [
    "ERROR: Only these are supported for NemoGuardrails:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed313f81-82a4-4ca9-a0ca-07a82e750b0d",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_google_vertexai import VertexAI\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1f9fa-bcdc-4cee-b908-ae74acef0484",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3b06c-4c81-42f0-9be4-d44699272af4",
   "metadata": {},
   "source": [
    "```python\n",
    "elif model_config.engine == \"nvidia_ai_endpoints\" or model_config.engine == \"nim\":\n",
    "    try:\n",
    "        from ._langchain_nvidia_ai_endpoints_patch import ChatNVIDIA\n",
    "\n",
    "        # Check the version\n",
    "        package_version = version(\"langchain_nvidia_ai_endpoints\")\n",
    "\n",
    "        if _parse_version(package_version) < (0, 2, 0):\n",
    "            raise ValueError(\n",
    "                \"langchain_nvidia_ai_endpoints version must be 0.2.0 or above.\"\n",
    "                \" Please upgrade it with `pip install langchain-nvidia-ai-endpoints --upgrade`.\"\n",
    "            )\n",
    "        return ChatNVIDIA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca19c5-d8ab-4ff8-b2f4-37d121e953da",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11de70f-ba92-4277-86ee-8d18d3b814e4",
   "metadata": {},
   "source": [
    "## 15. LangFuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44a2ca-27ea-4fc4-ac10-54002362d13e",
   "metadata": {},
   "source": [
    "- Open source llm engineering platform\n",
    "- alternative to langsmith\n",
    "- can be self deployed (crucial when we want strict data privacy (fully local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c637b5e-249b-451a-8698-d4dccda46aed",
   "metadata": {},
   "source": [
    "## 16. Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bab71f-2bb1-4732-b979-c4a764422693",
   "metadata": {},
   "source": [
    "- LLMs don't have all information available (real time information, company's internal data, etc..)\n",
    "- RAG and letting LLM write SQL queries is not enough\n",
    "- Tool calling allows getting Real time/ Near time data\n",
    "- Perfect to get data from an API (since it provides an interface for structured input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bdf77-6cb1-4b10-b44a-f2768386e9bd",
   "metadata": {},
   "source": [
    "old langchain_google_genai version = 2.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695f422-4935-4ff7-84f5-6b6235f002c2",
   "metadata": {},
   "source": [
    "`!pip uninstall langchain_experimental guardrails-ai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7b59548-f873-4ded-81fa-c8eaddb3d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_google_genai --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b5f941-bed0-4c18-86bf-eecbe0db6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-google-genai\n",
      "Version: 2.0.9\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\HP\\anaconda3\\Lib\\site-packages\n",
      "Requires: filetype, google-generativeai, langchain-core, pydantic\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a7fdb6-b67c-4bb2-a4ce-b1237fde9b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have real-time access to specific weather data, including historical weather.  To get the weather information for Chennai from yesterday, you would need to consult a weather website or app that archives past weather data.  Some popular options include:\\n\\n* **AccuWeather:**  Often has detailed historical weather information.\\n* **Weather Underground (wunderground.com):**  Known for its historical weather data.\\n* **Google Weather:** Search \"weather Chennai yesterday\" on Google.\\n* **Local news websites or apps:**  Indian news outlets often have weather sections.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from keys import gemini_api_key, api_key\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=gemini_api_key)\n",
    "llm.invoke(\"How was the weather in Chennai yesterday?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50dce0-15de-4ec6-aedb-51c44365382e",
   "metadata": {},
   "source": [
    "- In Langchain there are 2 ways to declare tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31217df2-d4e5-499f-9540-a656d713c4d5",
   "metadata": {},
   "source": [
    "1. **Declaring tools as classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3bc3ce6-bbfa-4982-afda-142357b5dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class fake_weather_api(BaseModel):\n",
    "    \"\"\"Check the weather in a specified city.\"\"\"\n",
    "    city: str = Field(default=\"\", description=\"The name of the city where you want to check the weather.\")\n",
    "\n",
    "class outdoor_seating_availability(BaseModel):\n",
    "    \"\"\"Check if outdoor seating is available at a specified restaurant in a given city.\"\"\"\n",
    "    city: str = Field(default=\"\", description=\"The name of the city where you want to check for outdoor seating availability.\")\n",
    "\n",
    "tools= [fake_weather_api, outdoor_seating_availability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcd63c-271d-49e0-8526-fa1e899c1c8e",
   "metadata": {},
   "source": [
    "2. **Using** `@tool` **decorator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889f0c2-8814-4e9d-91a4-f25cd7366bdd",
   "metadata": {},
   "source": [
    "- tool decorator must be put on top of the tool function\n",
    "- the tool function MUST have a doc string in the above mentioned format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dddae10f-800f-4b03-a78b-9fb1c08b97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    '''\n",
    "    Check the weather in a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city where you want to check the weather.\n",
    "\n",
    "    Returns:\n",
    "        str: A description of the current weather in the specified city.\n",
    "    '''\n",
    "    return \"Rainy, 21C\"\n",
    "\n",
    "@tool\n",
    "def outdoor_seating_availability(city: str) -> str:\n",
    "    '''\n",
    "    Check if outdoor seating is available at a specified restaurant in a given city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city where you want to check for outdoor seating availability.\n",
    "\n",
    "    Returns:\n",
    "        str: A message stating whether outdoor seating is available or not.\n",
    "    '''\n",
    "    return \"Outdoor seating is available.\"\n",
    "\n",
    "tools= [fake_weather_api, outdoor_seating_availability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea442a5d-57f4-42fe-8da2-ec4703cb83db",
   "metadata": {},
   "source": [
    "Both the above implementation produce the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "968211af-e87d-405e-aa32-75e273dd9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools= llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7b067b2-854c-4ef2-886f-2a6b196a76db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'fake_weather_api', 'arguments': '{\"city\": \"Chennai\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-3a7dee77-abda-494d-9014-87a80d0f412c-0', tool_calls=[{'name': 'fake_weather_api', 'args': {'city': 'Chennai'}, 'id': '44f69378-a28f-49d1-8ec2-a44747367851', 'type': 'tool_call'}], usage_metadata={'input_tokens': 135, 'output_tokens': 7, 'total_tokens': 142, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"What is the weather today in Chennai?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bf15f-6654-402a-94f8-f3143a6af7e7",
   "metadata": {},
   "source": [
    "- In AI Message, the content is an empty string\n",
    "- Instead, we have a special argument \"tool_calls\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4e79d-1845-4608-b8be-1caa8ebf1daa",
   "metadata": {},
   "source": [
    "`tool_calls=[{'name': 'fake_weather_api', 'args': {'city': 'Chennai'}, 'id': 'da45d986-c7bb-443b-83a0-3bc16c18157b', 'type': 'tool_call'}]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17db5d-ecd2-4712-97e8-b78ef9c7e9d4",
   "metadata": {},
   "source": [
    "- Some times, one tool call is not sufficient to answer the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "911c0d01-b67c-4c4d-9229-9a78c5b9348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fake_weather_api',\n",
       "  'args': {'city': 'Chennai'},\n",
       "  'id': 'c020dc3a-2f03-44ea-a575-6883b92c52c6',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'outdoor_seating_availability',\n",
       "  'args': {'city': 'Chennai'},\n",
       "  'id': 'b46e39ec-b586-4bc2-bb66-9e6cc1d32876',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"What is the weather today in Chennai? Do you still have seats outdoor available?\")\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ae43c-e411-4012-a561-3b95bb6aff4f",
   "metadata": {},
   "source": [
    "- Now how to work with those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09965e3e-7f1e-4c86-9c9d-422b2cb4622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        \"How will the weather be in Chennai today? I would like to eat outside if possible.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm_output = llm_with_tools.invoke(messages)\n",
    "messages.append(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20cc4da9-25cd-4ee7-aba8-9de2a7594e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How will the weather be in Chennai today? I would like to eat outside if possible.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  fake_weather_api (20cb1ff5-10a5-4043-8ed1-3f91db0f35d2)\n",
      " Call ID: 20cb1ff5-10a5-4043-8ed1-3f91db0f35d2\n",
      "  Args:\n",
      "    city: Chennai\n",
      "  outdoor_seating_availability (ce532635-4a26-46ab-af33-8daf3b38adf8)\n",
      " Call ID: ce532635-4a26-46ab-af33-8daf3b38adf8\n",
      "  Args:\n",
      "    city: Chennai\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e1b9363-5e51-4f57-8209-70eab1067114",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"fake_weather_api\" : fake_weather_api,\n",
    "    \"outdoor_seating_availability\" : outdoor_seating_availability\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "88a6adba-8466-4d3f-8d9b-12d7a170bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in llm_output.tool_calls:\n",
    "    tool = tool_mapping[tool_call[\"name\"]]\n",
    "    tool_output = tool.invoke(tool_call)\n",
    "    # messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "    messages.append(ToolMessage(tool_output, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "04c1885f-2320-4e9b-8411-e90d6448bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How will the weather be in Chennai today? I would like to eat outside if possible.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  fake_weather_api (20cb1ff5-10a5-4043-8ed1-3f91db0f35d2)\n",
      " Call ID: 20cb1ff5-10a5-4043-8ed1-3f91db0f35d2\n",
      "  Args:\n",
      "    city: Chennai\n",
      "  outdoor_seating_availability (ce532635-4a26-46ab-af33-8daf3b38adf8)\n",
      " Call ID: ce532635-4a26-46ab-af33-8daf3b38adf8\n",
      "  Args:\n",
      "    city: Chennai\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: fake_weather_api\n",
      "\n",
      "content='Rainy, 21C' name='fake_weather_api' tool_call_id='20cb1ff5-10a5-4043-8ed1-3f91db0f35d2'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: outdoor_seating_availability\n",
      "\n",
      "content='Outdoor seating is available.' name='outdoor_seating_availability' tool_call_id='ce532635-4a26-46ab-af33-8daf3b38adf8'\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e1ba14e0-8640-45e9-80ad-ccb76aaa24ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in Chennai is Rainy with a temperature of 21C. Outdoor seating is available.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27df829-af98-4de9-8be4-f2253386689e",
   "metadata": {},
   "source": [
    "#### Tool calling with a live API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e71ad-4006-49e7-9d76-5f5bf4f87856",
   "metadata": {},
   "source": [
    "Run this locally:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af01404-28d3-4029-992a-bd3a3611625a",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    weather: str = \"\"\n",
    "\n",
    "\n",
    "class OutdoorSeatingResponse(BaseModel):\n",
    "    outdoor_seating: str = \"\"\n",
    "\n",
    "\n",
    "weather_data = {\n",
    "    \"Rainytown\": \"Rainy, 25°C\",\n",
    "    \"Sunland\": \"Sunny, 28°C\",\n",
    "    \"Windyworld\": \"Windy, 22°C\",\n",
    "    \"Cloudycity\": \"Cloudy, 22°C\"\n",
    "}\n",
    "\n",
    "outdoor_seating_data = {\n",
    "    \"Rainytown\": \"Outdoor seating is not available\",\n",
    "    \"Sunland\": \"Outdoor seating is available\",\n",
    "    \"Windyworld\": \"Outdoor seating is available\",\n",
    "    \"Cloudycity\": \"Outdoor seating is not available\"\n",
    "}\n",
    "\n",
    "\n",
    "@app.get(\"/weather/{city}\", response_model=WeatherResponse)\n",
    "async def get_weather(city: str):\n",
    "    # city_lower = city.lower()\n",
    "    return {\n",
    "        \"weather\": weather_data.get(city_lower, \"Weather info not available\")\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/outdoor-seating/{city}\", response_model=OutdoorSeatingResponse)\n",
    "async def get_outdoor_seating(city: str):\n",
    "    # city_lower = city.lower()\n",
    "    return {\n",
    "        \"outdoor_seating\": outdoor_seating_data.get(city_lower, \"Outdoor seating info not available\")\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=1906)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021f1688-4c5d-467f-bd42-79e0de6a26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "import httpx\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from keys import gemini_api_key, api_key\n",
    "\n",
    "@tool\n",
    "def our_weather_api(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get Weather information about the specified city from a FastAPI endpoint on localhost:1906 \n",
    "    \"\"\"\n",
    "    response = httpx.get(f\"http://localhost:1906/weather/{city}\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"weather\", \"Weather information not available\")\n",
    "    else:\n",
    "        return \"Failed to get weather information\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def our_seating_api(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if outdoor seating is available in a specified city from a FastAPI endpoint on localhost:1906\n",
    "    \"\"\"\n",
    "    response = httpx.get(f\"http://localhost:1906/outdoor-seating/{city}\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"outdoor_seating\", \"Outdoor seating information not available\")\n",
    "    else:\n",
    "        return \"Failed to get outdoor seating information\"\n",
    "\n",
    "\n",
    "api_tools = [our_weather_api, our_seating_api]\n",
    "tool_mapping = {\n",
    "    \"our_weather_api\": our_weather_api,\n",
    "    \"our_seating_api\": our_seating_api\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0657f3-5f6b-4adf-b942-79aae43055a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_llm_and_tools(human_message: str):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=gemini_api_key)\n",
    "    model = llm.bind_tools(api_tools)\n",
    "\n",
    "    messages = [HumanMessage(content=human_message)]\n",
    "    llm_output = model.invoke(messages)\n",
    "    messages.append(llm_output)\n",
    "\n",
    "    print(\"Tool calls:\", llm_output.tool_calls)\n",
    "    for tool_call in llm_output.tool_calls:\n",
    "        tool = tool_mapping[tool_call[\"name\"]]\n",
    "        if tool:\n",
    "            tool_output = tool.invoke(tool_call[\"args\"])\n",
    "            print(\"Tool output:\", tool_output)\n",
    "            messages.append(ToolMessage(tool_output, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "    final_response = model.invoke(messages)\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27499f29-833c-4448-8fe9-a310723b62e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [{'name': 'our_weather_api', 'args': {'city': 'Windyworld'}, 'id': '889f11df-4691-424f-ade0-2a2e849777fc', 'type': 'tool_call'}, {'name': 'our_seating_api', 'args': {'city': 'Windyworld'}, 'id': 'da3bf74a-a724-4064-b5b4-6cf67ad848d1', 'type': 'tool_call'}]\n",
      "Tool output: Windy, 22°C\n",
      "Tool output: Outdoor seating is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The weather in Windyworld is windy and 22°C. Outdoor seating is available.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-8bb610f8-0235-4eea-b8f9-79c4d4c2148f-0', usage_metadata={'input_tokens': 217, 'output_tokens': 20, 'total_tokens': 237, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_with_llm_and_tools(\"How will the weather be in Windyworld? I would like to eat outside. Is it possible?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa3f8d-979f-4007-a465-5d9e36e8fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
