{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84832d19-46ae-476d-b1c2-6dc4dad8eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai langchain_google_genai langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e436d0ac-bf39-4be8-a8be-63fd59d5d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 + 42 = **97** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "'''\n",
    "or use these instead of ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "'''\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from keys import gemini_api_key\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]= gemini_api_key\n",
    "\n",
    "#llm= ChatOpenAI(temperature= 0, model= \"gpt-3.5-turbo\")\n",
    "llm= ChatGoogleGenerativeAI(model= \"gemini-1.5-pro-latest\")\n",
    "\n",
    "#template1\n",
    "prompt= PromptTemplate(\n",
    "    input_variables= ['x', 'y'],\n",
    "    template= \"what is {x} + {y}?\"\n",
    ")\n",
    "\n",
    "chain= prompt | llm\n",
    "result= chain.invoke(input= {\"x\": 55, \"y\": 42})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8da57e-1fd5-49c7-86c8-679fb9fdf314",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dd9ec0-1315-47c7-a082-2ce57ff96a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Answer in one word. What shape is the moon?')]\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an expert in science. Help the audience solve their issues. Give straight on to the point answers. Don't justify them.\"), HumanMessage(content='Which floats on top of the other? water or oil?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#template2\n",
    "template= \"Answer in one word. What {attribute} is the {input}?\"\n",
    "prompt_template= ChatPromptTemplate.from_template(template)\n",
    "prompt= prompt_template.invoke({\"attribute\": \"shape\", \"input\": \"moon\"})\n",
    "print(prompt)\n",
    "print()\n",
    "#template3\n",
    "messages= [\n",
    "    (\"system\", \"You are an expert in science. Help the audience solve their issues. Give straight on to the point answers. Don't justify them.\"),\n",
    "    (\"human\", \"Which floats on top of the other? {input1} or {input2}?\")\n",
    "]\n",
    "prompt_template2= ChatPromptTemplate.from_messages(messages)\n",
    "prompt2= prompt_template2.invoke({\"input1\": \"water\", \"input2\": \"oil\"})\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95128be0-26cb-4fa0-8537-f333238e4ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "13 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "messages= [\n",
    "    SystemMessage(content= \"Solve the following math problems\"),\n",
    "    HumanMessage(content= \"What is 9 + 10?\"),\n",
    "    AIMessage(content= \"19\"),\n",
    "    HumanMessage(content= \"What is 10 + 3?\")\n",
    "]\n",
    "result= llm.invoke(messages)\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae7b2e54-3a2e-48bd-889a-14f4bc6f51f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the largest desert?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: That depends on what kind of desert you're talking about! \n",
      "\n",
      "* **Largest hot desert:** The **Sahara Desert** in North Africa is the largest hot desert in the world, covering a massive 9.2 million square kilometers (3.6 million square miles). \n",
      "\n",
      "* **Largest desert overall:**  The **Antarctic Polar Desert** is actually the largest desert on Earth, encompassing 14 million square kilometers (5.4 million square miles) of ice and snow. Polar deserts are defined by their low precipitation, similar to hot deserts, but are much colder.\n",
      "\n",
      "So, the Sahara wins for largest *hot* desert, but Antarctica takes the crown for the largest desert *overall*. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  why did you assign a crown?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: That was just a figure of speech! I was trying to make the explanation a little more engaging. 😊 \n",
      "\n",
      "I sometimes use metaphors and other literary devices to make my responses more interesting and easier to understand. \n",
      "\n",
      "Would you prefer I stick to a more straightforward, factual tone? I'm happy to adjust my communication style to your preference. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "#Real time chat application\n",
    "\n",
    "chat_history= []\n",
    "\n",
    "system_message= SystemMessage(content= \"You are a helpful AI assistant\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "while True:\n",
    "    query= input(\"You: \")\n",
    "    if query.lower()== \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content= query))\n",
    "    result= llm.invoke(chat_history)\n",
    "    response= result.content\n",
    "    chat_history.append(AIMessage(content= response))\n",
    "    print(\"AI: {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828ffbf4-d53c-4c27-980e-4ed65a0c78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Message History\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI assistant'),\n",
       " HumanMessage(content='What is the largest desert?'),\n",
       " AIMessage(content=\"That depends on what kind of desert you're talking about! \\n\\n* **Largest hot desert:** The **Sahara Desert** in North Africa is the largest hot desert in the world, covering a massive 9.2 million square kilometers (3.6 million square miles). \\n\\n* **Largest desert overall:**  The **Antarctic Polar Desert** is actually the largest desert on Earth, encompassing 14 million square kilometers (5.4 million square miles) of ice and snow. Polar deserts are defined by their low precipitation, similar to hot deserts, but are much colder.\\n\\nSo, the Sahara wins for largest *hot* desert, but Antarctica takes the crown for the largest desert *overall*. \\n\"),\n",
       " HumanMessage(content='why did you assign a crown?'),\n",
       " AIMessage(content=\"That was just a figure of speech! I was trying to make the explanation a little more engaging. 😊 \\n\\nI sometimes use metaphors and other literary devices to make my responses more interesting and easier to understand. \\n\\nWould you prefer I stick to a more straightforward, factual tone? I'm happy to adjust my communication style to your preference. \\n\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Chat Message History\")\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dc21a-8d82-4a8c-9218-31f9fb54f78f",
   "metadata": {},
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3295377-d18e-4e1e-a653-c038b958cf31",
   "metadata": {},
   "source": [
    "### StrOutputParser: removes all the meta data from llm's output and returns only the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7f2c1f-5f26-4250-992c-02b17c23ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oil floats on top of water. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain= prompt_template2 | llm | StrOutputParser()\n",
    "\n",
    "result= chain.invoke({\"input1\": \"water\", \"input2\": \"oil\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14533f1a-41f8-40c2-86d8-f205dc8a21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OIL*FLOATS*ON*TOP*OF*WATER.*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "uppercase_op= RunnableLambda(lambda x: x.upper())\n",
    "replace_space= RunnableLambda(lambda x: x.replace(\" \", \"*\"))\n",
    "\n",
    "chain= prompt_template2 | llm | StrOutputParser() | uppercase_op | replace_space\n",
    "result= chain.invoke({\"input1\": \"water\", \"input2\": \"oil\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fa536-c48e-46c8-bf7b-effdcc31a900",
   "metadata": {},
   "source": [
    "#### Behind the scenes of the langchain chaining:<br>\n",
    "RunnableSequence: sequence of runnables where the output of each is the input of the next.<br>\n",
    "Implemented using | pipe operator.. both the sides of the pipe should be a Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea99cf2-e17a-4369-a352-b4dc8da388b7",
   "metadata": {},
   "source": [
    "## Parallel Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "338e8be9-c18b-45f6-b0df-716cae4937f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(model= \"gemini-1.5-pro-latest\")\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"List the main features of this product: {product}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def list_advs(features):\n",
    "    advs_template= ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are and expert product reviewer.\"),\n",
    "            (\"human\", \"Given these features: {features}, list their advantages\")\n",
    "        ]\n",
    "    )\n",
    "    return advs_template.format_prompt(features= features)\n",
    "\n",
    "def list_dis_advs(features):\n",
    "    dis_advs_template= ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are and expert product reviewer.\"),\n",
    "            (\"human\", \"Given these features: {features}, list their disadvantages\")\n",
    "        ]\n",
    "    )\n",
    "    return dis_advs_template.format_prompt(features= features)\n",
    "\n",
    "def combine(advs, dis_advs):\n",
    "    return f\"Advantages:\\n{advs}\\n\\nDisadvantages:\\n{dis_advs}\"\n",
    "\n",
    "branch1= RunnableLambda(lambda x: list_advs(x)) | llm | StrOutputParser()\n",
    "branch2= RunnableLambda(lambda x: list_dis_advs(x)) | llm | StrOutputParser()\n",
    "\n",
    "branch= RunnableParallel(branches= {\"advs\": branch1, \"dis_advs\": branch2})\n",
    "\n",
    "concat= RunnableLambda(lambda x: combine(x[\"branches\"][\"advs\"], x[\"branches\"][\"dis_advs\"]))\n",
    "\n",
    "chain= prompt | llm | StrOutputParser() | branch | concat\n",
    "\n",
    "result= chain.invoke({\"product\": \"HP Z-book firefly 14 g10; Processor: Intel Core i7-1280P, Graphics: Intel Iris Xe Graphics, RAM: 32GB, Storage: 256GB PCIe NVMe SSD, Display: 1920x1080 FHD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ccab9a1-94f5-4872-aa1f-080dc22e1b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Advantages:\n",
       "## Advantages of the HP ZBook Firefly 14 G10 based on your provided features:\n",
       "\n",
       "Here are the advantages broken down by category:\n",
       "\n",
       "**Performance:**\n",
       "\n",
       "* **Strong Processor for Productivity:** The Intel Core i7-1280P offers a great balance of power and efficiency. You'll experience smooth performance in demanding applications and multitasking.\n",
       "* **Generous RAM for Smooth Multitasking:** 32GB of RAM is more than enough for most professional workloads, allowing you to run multiple applications, browser tabs, and virtual machines simultaneously without slowdowns.\n",
       "* **Fast Storage for Enhanced Responsiveness:** The 256GB PCIe NVMe SSD ensures rapid boot times, quick application loading, and snappy file transfers, significantly improving your overall productivity.\n",
       "\n",
       "**Display & Design:**\n",
       "\n",
       "* **Sharp and Vibrant Display:** The 1920x1080 FHD display provides good clarity and visual quality for productivity tasks, presentations, and media consumption.\n",
       "\n",
       "**Other:**\n",
       "\n",
       "* **ZBook Reliability and Durability:**  The ZBook series is known for its robust build quality, reliability, and ISV certifications, making it a dependable choice for professionals.\n",
       "* **Portability for Professionals on the Go:**  The \"Firefly\" designation implies a lightweight and compact design, making it easy to carry and use while traveling or working remotely.\n",
       "\n",
       "**Overall:**\n",
       "\n",
       "This configuration of the HP ZBook Firefly 14 G10 offers a compelling package for professionals who prioritize a balance of performance and portability. It excels in everyday productivity tasks, multitasking, and handling demanding applications on the go. \n",
       "\n",
       "\n",
       "Disadvantages:\n",
       "Here are the disadvantages of the HP ZBook Firefly 14 G10 configuration, based on your provided details:\n",
       "\n",
       "* **Limited Storage:** The 256GB SSD, while fast, might be insufficient for professionals dealing with large files (e.g., high-resolution images, videos, complex datasets) or those who need ample local storage space. \n",
       "* **Insufficient Graphics for Demanding Tasks:** The integrated Intel Iris Xe Graphics, while suitable for everyday tasks and light creative work, will likely fall short for professionals requiring heavy graphical horsepower (e.g., CAD designers, video editors, 3D modelers). \n",
       "* **Display Resolution (Potentially):** While the 1920x1080 FHD resolution is decent for a 14-inch screen, some professionals might desire a higher resolution display (e.g., 1440p or 4K) for sharper visuals and more screen real estate, especially for tasks involving detailed work.\n",
       "\n",
       "**In addition to the points you listed, here are some potential drawbacks to consider, depending on the specific use case and user preferences:**\n",
       "\n",
       "* **Price:** ZBook models are often priced at a premium compared to consumer-grade laptops with similar specs.  \n",
       "* **Battery Life:** While the processor is designed for efficiency, real-world battery life can vary significantly based on usage patterns, screen brightness, and other factors. Professionals needing extended unplugged use might find the battery life limiting.\n",
       "* **Port Selection:**  The specific ports offered on this configuration are not mentioned.  Limited port selection could be a drawback for some professionals. \n",
       "\n",
       "It's important to note that these disadvantages might not be deal-breakers for every user. The overall value of this HP ZBook Firefly 14 G10 configuration depends heavily on the specific needs and priorities of the professional using it. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f130c-01e6-4770-b00a-fecc00db0010",
   "metadata": {},
   "source": [
    "## Chaining Branching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225f2f8-64a0-4607-8402-5752f12e4df2",
   "metadata": {},
   "source": [
    "#### Difference between RunnableParallel and RunnableBranch'''\n",
    "\n",
    ">parallel: all branches will be executed simultaneously<br>\n",
    ">branchg: only one branch (or more, depending on a condition) is/are executed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5f0cf7e-6a60-4183-8368-c9a4b67f8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "DEFINE ALL THE PROMPTS\n",
    "'''\n",
    "positive_feedback= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"Generate a thank you note for this positive feedback: {feedback}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "negative_feedback= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"Generate a response which addresses this negative feedback: {feedback}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "neutral_feedback= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"Generate a request for more details for this neutral feedback: {feedback}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "escalate_feedback= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"Generate a message to escalate this feedback to a human agent: {feedback}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "main_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Classify the sentiment of this feedback as positive, negative, neutral or escalate: {feedback}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "#similiar to switch case; the last statement gets executed when all conditions fail\n",
    "# (\n",
    "#      condition ------------> if true, <this chain will execute>  #..other chains wont be executed\n",
    "# )\n",
    "branches= RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x, positive_feedback | llm | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x, negative_feedback | llm | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x, neutral_feedback | llm | StrOutputParser()\n",
    "    ),\n",
    "    escalate_feedback | llm | StrOutputParser()\n",
    "    \n",
    ")\n",
    "\n",
    "chain= main_prompt| llm | StrOutputParser() | branches\n",
    "\n",
    "result= chain.invoke({\"feedback\": \"This product is terrible, Not worth the money I spent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06994e65-dfc2-4fc4-a0e0-10e119357f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Addressing the Feedback: \"This product is terrible. Not worth the money I spent.\"\n",
       "\n",
       "We are incredibly sorry to hear about your negative experience with our product. We understand your frustration and disappointment, especially with you feeling like the product wasn't worth the investment. \n",
       "\n",
       "Your feedback is valuable to us, and we appreciate you taking the time to share your thoughts. To better understand the situation and see how we can help, could you please tell us more about what specifically you found terrible about the product? \n",
       "\n",
       "Knowing more about your experience will allow us to:\n",
       "\n",
       "1. **Address your concerns directly:** We want to see if there's anything we can do to improve your experience with the product, whether it's offering troubleshooting advice, suggesting workarounds, or considering a refund.\n",
       "2. **Improve our product:** Your feedback helps us identify areas where the product falls short and allows us to make necessary improvements for future customers.\n",
       "\n",
       "We want to regain your trust and show you that we are committed to providing a positive experience with our products. Please feel free to contact us directly at [Contact Information] so we can discuss this further. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b097ec-a001-40d7-940e-06ca6da51dde",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b328462-ab9e-433c-99f6-6e2d6bc1d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dda2b-0c44-4332-ace4-2555461f5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d34c5f2-4566-456b-bea0-510c9e91212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3795a1e5-a901-4f2f-85d3-4f289ca8b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4948b738-5bec-44c4-bb92-3d57d1e35693",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir= os.path.dirname(os.path.abspath(\"langchain-tutorial.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b254a70d-b105-4294-9489-0830b7e529d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= os.path.join(cur_dir, \"color-psychology.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dde6def-1b9a-45b8-a597-9a34ba8c40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_directory= os.path.join(cur_dir, \"db\", \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0997554b-78d7-47b5-88d2-ec857bb96645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 288, which is longer than the specified 200\n",
      "Created a chunk of size 261, which is longer than the specified 200\n",
      "Created a chunk of size 311, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 277, which is longer than the specified 200\n",
      "Created a chunk of size 266, which is longer than the specified 200\n",
      "Created a chunk of size 228, which is longer than the specified 200\n",
      "Created a chunk of size 306, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 201, which is longer than the specified 200\n",
      "Created a chunk of size 267, which is longer than the specified 200\n",
      "Created a chunk of size 355, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(persistent_directory):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError()\n",
    "    loader= TextLoader(file_path)\n",
    "    documents= loader.load() \n",
    "\n",
    "    text_splitter= CharacterTextSplitter(chunk_size= 200, chunk_overlap= 0)\n",
    "    docs= text_splitter.split_documents(documents)\n",
    "\n",
    "    db= Chroma.from_documents(\n",
    "        docs, embeddings, \n",
    "        persist_directory= persistent_directory,   \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "else:\n",
    "    print(\"Vector Store already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5284907-7250-4b44-95ca-3afe28aefa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3a770d-67ab-41be-936e-e9e9c8e3470a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Color Psychology â€” as previously discussed, the world of graphic design and color theory are things that cannot be separated. Color theory is certainly very useful for creating good designs by paying attention to visual concepts that are attractive to the eye.', metadata={'source': 'C:\\\\Users\\\\HP\\\\Desktop\\\\chatbot\\\\color-psychology.txt'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f50f6-272e-417f-8d19-a5efb51cb671",
   "metadata": {},
   "source": [
    "#### Similiarity searching: whichever chunk resembles most similiear to the query shows up on top..<br>(and later we take top k and give it to llm to make inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0c1591-7186-410c-87a8-f038907fa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"How a sad event is depicted?.\"\n",
    "retriever= db.as_retriever(\n",
    "    search_type= \"similarity_score_threshold\",\n",
    "    search_kwargs= {\"k\": 3, \"score_threshold\": 0.1}\n",
    ")\n",
    "relevant_docs= retriever.invoke(query);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23165347-4bca-4258-935c-73c5c5edf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      " Showing a picture with a sad impression, a painter will give a little boredom to the picture. It can be seen from the paintings depicting a sad event, most of them have a lot of color, but there is little saturation, it could be said that the saturation is low.\n",
      "\n",
      "Source: C:\\Users\\HP\\Desktop\\chatbot\\color-psychology.txt\n",
      "\n",
      "Document 2:\n",
      " That way, the colors used must also be appropriate and appropriate for the design created. We cannot possibly give a dominant black color to an image that shows a cheerful impression, nor can we give a bright dominant color such as blue, red, pink to an image that shows a sad impression.\n",
      "\n",
      "Source: C:\\Users\\HP\\Desktop\\chatbot\\color-psychology.txt\n",
      "\n",
      "Document 3:\n",
      " So, what effect does black have on a personâ€™s psychology? What does the color black mean? Black gives a formal and exclusive impression. But on the other hand, black can also be interpreted as the color of darkness or sadness.\n",
      "\n",
      "Source: C:\\Users\\HP\\Desktop\\chatbot\\color-psychology.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n {doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df9d4f9-50ab-4166-bc0d-ae56223c3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_directory= os.path.join(cur_dir, \"db\", \"chroma_db_with_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda9445-8521-4874-9449-1ab0d03ba319",
   "metadata": {},
   "source": [
    "#### Add metadata to the file (just change the meta data from \"filepath\" to \"filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c064fe-3170-4167-be4b-40f7ba8c6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(persistent_directory):\n",
    "    files= [f for f in os.listdir(cur_dir) if f.endswith(\".txt\")]\n",
    "    documents= []\n",
    "    for file in files:\n",
    "        file_path= os.path.join(cur_dir, file)\n",
    "        loader= TextLoader(file_path)\n",
    "        docs= loader.load()\n",
    "        for doc in docs:\n",
    "            #here is the important part\n",
    "            doc.metadata= {\"source\": file}\n",
    "            documents.append(doc)\n",
    "    text_splitter= CharacterTextSplitter(chunk_size= 1000, chunk_overlap= 0)\n",
    "    docs= text_splitter.split_documents(documents)\n",
    "\n",
    "    db= Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory= persistent_directory,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "else:\n",
    "    print(\"Vector store already exists\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c41e61-fb8a-4c4b-8023-6b3bcbdcbe28",
   "metadata": {},
   "source": [
    "#### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12a8e842-84d8-47e3-8464-1cff9e43ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color-psychology-kfp.txt\n",
      "color-psychology.txt\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    file_path= os.path.join(cur_dir, file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf6ad7-14df-4117-86a1-4420e03b2388",
   "metadata": {},
   "source": [
    "CharacterTextSplitter<br>\n",
    "NLTKTextSplitter<br>\n",
    "SpaCyTextSplitter<br>\n",
    "SentenceTransformersTokenTextSplitter<br>\n",
    "TextSplitter<br>\n",
    "TokenTextSplitter<br>\n",
    "RecursiveCharacterTextSplitter and many more...<br>\n",
    "\n",
    "**the size of the chunk determines the no of characters to be inside the chunk**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3604d58-fdf9-414e-8c67-599de8b80d4a",
   "metadata": {},
   "source": [
    "##Searching methods inside the vector database:\n",
    "similarity_search\n",
    "similarity_score_threshold\n",
    "max marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad0c3ab-3624-4fca-8afc-be376637eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"what is the color of the protagonist?\"\n",
    "\n",
    "retriever= db.as_retriever(\n",
    "    search_type= \"similarity\",\n",
    "    search_kwargs= {\"k\": 1}\n",
    ")\n",
    "relevant_docs= retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea493974-1df0-4c12-8444-8ee2e025884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Gold is a color thatâ€™s present since the very beginning of the film, and codes for both our heroic characters/scenes, and for positive emotions in general, particularly their use to resolve a characterâ€™s progression from a state of struggling with things like regret and self-worth. It frequently shows up with another positive-coded color - green - which I will get into later.\n",
      "\n",
      "Tigress against a golden background in the scene preceding Poâ€™s getting his ass handed to him by the Five, establishing her as a heroic/good guy character despite her ground-state mood of â€˜pissed offâ€™. The latter scene then shifts towards red when Shifu finally steps up to fight Po, possibly indicating Poâ€™s own power in refusing to quit, but also emphasizing just how pissed off this makes Shifu\n",
      "\n",
      "Poâ€™s dream sequence, which is coded almost entirely gold and red, indicating Poâ€™s desire to become a powerful and respected kung-fu hero.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49dc8938-f759-447a-9fe3-f1c5512f7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "content= \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "i_p= (\n",
    "    '''Please answer the question only using the given content. If you don't know, answer that you don't know. Don't make up content and DO NOT use any other content other than what is provided.\n",
    "    Content: {content}\\n\\n\n",
    "    Question: {query}\n",
    "    '''\n",
    ")\n",
    "\n",
    "prompt_template= ChatPromptTemplate.from_template(i_p)\n",
    "\n",
    "chain= prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec639b49-fc6b-42b2-9d1f-226bae8aa272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text states that gold is a color that codes for heroic characters, and that Po desires to become a kung fu hero. Therefore, the protagonist (Po) is associated with the color **gold**. \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"content\": content, \"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad85549-0bd2-4c1a-858e-4e42f719f987",
   "metadata": {},
   "source": [
    "# Conversation with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f058b21-895a-4fe4-8a58-8a1221346ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm= ChatGoogleGenerativeAI(model= \"gemini-1.5-pro-latest\")\n",
    "\n",
    "database= \"C:/Users/HP/Desktop/chatbot/db/chroma_db_with_metadata\"\n",
    "\n",
    "db= Chroma(persist_directory= database, \n",
    "           embedding_function= embeddings)\n",
    "\n",
    "retriever= db.as_retriever(\n",
    "    search_type= \"similarity\",\n",
    "    search_kwargs= {\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b924809-2e05-47e3-8d88-4d16213021dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt= (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just summarize and rephrase it if you can, otherwise return it as is.\" \n",
    ")\n",
    "\n",
    "contextualize_q_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_system_prompt= (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain= create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain= create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4d0c37d-558b-4e96-b140-51be3bd047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_chat():\n",
    "    print(\"Chat with AI. Type 'exit' if you are done\")\n",
    "    chat_history= []\n",
    "    while True:\n",
    "        query= input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        result= rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        chat_history.append(HumanMessage(content= query))\n",
    "        chat_history.append(SystemMessage(content= result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45afed87-ff3c-45f5-ad3c-4a93be2efff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what color is bugatti?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: This document does not contain the answer to what color a Bugatti is. This document discusses color theory in relation to the movie Kung Fu Panda. \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what color symbolises the villan?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected message with type <class 'langchain_core.messages.system.SystemMessage'> at the position 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m continual_chat()\n",
      "Cell \u001b[1;32mIn[69], line 9\u001b[0m, in \u001b[0;36mcontinual_chat\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Process the user's query through the retrieval chain\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history})\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Display the AI's response\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4588\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   4589\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4590\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4591\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4592\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2505\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2501\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2502\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2503\u001b[0m )\n\u001b[0;32m   2504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2505\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2507\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:469\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    466\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1599\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1595\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1596\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1597\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1598\u001b[0m         Output,\n\u001b[1;32m-> 1599\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1600\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m             config,\n\u001b[0;32m   1604\u001b[0m             run_manager,\n\u001b[0;32m   1605\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1606\u001b[0m         ),\n\u001b[0;32m   1607\u001b[0m     )\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1609\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:456\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    452\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    458\u001b[0m             patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[0;32m    459\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    460\u001b[0m         ),\n\u001b[0;32m    461\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3152\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   3139\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3140\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3141\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   3142\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3150\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3151\u001b[0m         ]\n\u001b[1;32m-> 3152\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3152\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3139\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3140\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3141\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   3142\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3150\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3151\u001b[0m         ]\n\u001b[1;32m-> 3152\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4588\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   4589\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4590\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4591\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4592\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\branch.py:212\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    213\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    214\u001b[0m             config\u001b[38;5;241m=\u001b[39mpatch_config(\n\u001b[0;32m    215\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbranch:default\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m             ),\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    218\u001b[0m         )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2507\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2506\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2507\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   2508\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:248\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    245\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    247\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    249\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    250\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    251\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    252\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    253\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    254\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    255\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    257\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    258\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:677\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    671\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    675\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    676\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:534\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    533\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    536\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    538\u001b[0m ]\n\u001b[0;32m    539\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 524\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    525\u001b[0m                 m,\n\u001b[0;32m    526\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    527\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    528\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    529\u001b[0m             )\n\u001b[0;32m    530\u001b[0m         )\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:749\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 749\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    750\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    751\u001b[0m         )\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:758\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    747\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    757\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 758\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    759\u001b[0m         messages,\n\u001b[0;32m    760\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    761\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    762\u001b[0m         functions\u001b[38;5;241m=\u001b[39mfunctions,\n\u001b[0;32m    763\u001b[0m         safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[0;32m    764\u001b[0m         tool_config\u001b[38;5;241m=\u001b[39mtool_config,\n\u001b[0;32m    765\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    766\u001b[0m     )\n\u001b[0;32m    767\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    768\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    770\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m    771\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m    772\u001b[0m     )\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:901\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._prepare_request\u001b[1;34m(self, messages, stop, tools, functions, safety_settings, tool_config, generation_config)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m functions:\n\u001b[0;32m    899\u001b[0m     formatted_tools \u001b[38;5;241m=\u001b[39m [convert_to_genai_function_declarations(functions)]\n\u001b[1;32m--> 901\u001b[0m system_instruction, history \u001b[38;5;241m=\u001b[39m _parse_chat_history(\n\u001b[0;32m    902\u001b[0m     messages,\n\u001b[0;32m    903\u001b[0m     convert_system_message_to_human\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_system_message_to_human,\n\u001b[0;32m    904\u001b[0m )\n\u001b[0;32m    905\u001b[0m formatted_tool_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool_config:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:435\u001b[0m, in \u001b[0;36m_parse_chat_history\u001b[1;34m(input_messages, convert_system_message_to_human)\u001b[0m\n\u001b[0;32m    422\u001b[0m         parts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    423\u001b[0m             Part(\n\u001b[0;32m    424\u001b[0m                 function_response\u001b[38;5;241m=\u001b[39mFunctionResponse(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m             )\n\u001b[0;32m    433\u001b[0m         ]\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    436\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected message with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at the position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         )\n\u001b[0;32m    439\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(Content(role\u001b[38;5;241m=\u001b[39mrole, parts\u001b[38;5;241m=\u001b[39mparts))\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m system_instruction, messages\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected message with type <class 'langchain_core.messages.system.SystemMessage'> at the position 2."
     ]
    }
   ],
   "source": [
    "continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e8219-bdc3-41b3-86b0-1fadab496b6e",
   "metadata": {},
   "source": [
    "## Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e4ab4dc-7d0d-45f1-9538-223222f1e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What time it is?\n",
      "Thought: I need to use the Time tool to get the current time.\n",
      "Action: Time\n",
      "Action Input: \u001b[0m\u001b[36;1m\u001b[1;3m05:05 PM\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: What time it is?\n",
      "Thought: I need to use the Time tool to get the current time.\n",
      "Action: Time\n",
      "Action Input: \u001b[0m\u001b[36;1m\u001b[1;3m05:05 PM\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to use the Time tool to get the current time.\n",
      "Action: Time\n",
      "Action Input: \u001b[0m\u001b[36;1m\u001b[1;3m05:05 PM\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mQuestion: What time it is?\n",
      "Thought: I need to use the Time tool to get the current time.\n",
      "Action: Time\n",
      "Action Input: \u001b[0m\u001b[36;1m\u001b[1;3m05:05 PM\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 53\u001b[0m\n\u001b[0;32m     40\u001b[0m agent\u001b[38;5;241m=\u001b[39m create_react_agent(\n\u001b[0;32m     41\u001b[0m     llm\u001b[38;5;241m=\u001b[39m llm,\n\u001b[0;32m     42\u001b[0m     tools\u001b[38;5;241m=\u001b[39m tools,\n\u001b[0;32m     43\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m prompt,\n\u001b[0;32m     44\u001b[0m     stop_sequence\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m agent_executor\u001b[38;5;241m=\u001b[39m AgentExecutor\u001b[38;5;241m.\u001b[39mfrom_agent_and_tools(\n\u001b[0;32m     48\u001b[0m     agent\u001b[38;5;241m=\u001b[39m agent,\n\u001b[0;32m     49\u001b[0m     tools\u001b[38;5;241m=\u001b[39m tools,\n\u001b[0;32m     50\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     51\u001b[0m )\n\u001b[1;32m---> 53\u001b[0m response\u001b[38;5;241m=\u001b[39m agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat time it is?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1433\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1433\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1434\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1435\u001b[0m         color_mapping,\n\u001b[0;32m   1436\u001b[0m         inputs,\n\u001b[0;32m   1437\u001b[0m         intermediate_steps,\n\u001b[0;32m   1438\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1439\u001b[0m     )\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1442\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1443\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1139\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1132\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1139\u001b[0m         [\n\u001b[0;32m   1140\u001b[0m             a\n\u001b[0;32m   1141\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1142\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1143\u001b[0m                 color_mapping,\n\u001b[0;32m   1144\u001b[0m                 inputs,\n\u001b[0;32m   1145\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1146\u001b[0m                 run_manager,\n\u001b[0;32m   1147\u001b[0m             )\n\u001b[0;32m   1148\u001b[0m         ]\n\u001b[0;32m   1149\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1139\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1132\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1139\u001b[0m         [\n\u001b[0;32m   1140\u001b[0m             a\n\u001b[0;32m   1141\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1142\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1143\u001b[0m                 color_mapping,\n\u001b[0;32m   1144\u001b[0m                 inputs,\n\u001b[0;32m   1145\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1146\u001b[0m                 run_manager,\n\u001b[0;32m   1147\u001b[0m             )\n\u001b[0;32m   1148\u001b[0m         ]\n\u001b[0;32m   1149\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1167\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1168\u001b[0m         intermediate_steps,\n\u001b[0;32m   1169\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1171\u001b[0m     )\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:398\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2885\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2880\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2881\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2882\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2883\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2884\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2885\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2872\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2867\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2868\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2869\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2871\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2872\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2873\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2874\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2875\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2876\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2877\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1868\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1867\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1868\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1870\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2834\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   2831\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2832\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 2834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2835\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1163\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1160\u001b[0m final: Input\n\u001b[0;32m   1161\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1172\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4799\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   4794\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4795\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   4796\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4797\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   4798\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 4799\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   4800\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4802\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4803\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1181\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:343\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    337\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    338\u001b[0m         e,\n\u001b[0;32m    339\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    340\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    341\u001b[0m         ),\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:323\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:839\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._stream\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    825\u001b[0m     messages,\n\u001b[0;32m    826\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    832\u001b[0m )\n\u001b[0;32m    833\u001b[0m response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    834\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    835\u001b[0m     generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstream_generate_content,\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    837\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m    838\u001b[0m )\n\u001b[1;32m--> 839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    840\u001b[0m     _chat_result \u001b[38;5;241m=\u001b[39m _response_to_result(chunk, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    841\u001b[0m     gen \u001b[38;5;241m=\u001b[39m cast(ChatGenerationChunk, _chat_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:116\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# If the stream has already returned data, we cannot recover here.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:960\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    956\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[0;32m    957\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     )\n\u001b[1;32m--> 960\u001b[0m _common\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcondition\u001b[38;5;241m.\u001b[39mwait, _response_ready)\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\grpc\\_common.py:156\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[1;32m--> 156\u001b[0m         _wait_once(wait_fn, MAXIMUM_WAIT_TIMEOUT, spin_cb)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\grpc\\_common.py:116\u001b[0m, in \u001b[0;36m_wait_once\u001b[1;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(\n\u001b[0;32m    112\u001b[0m     wait_fn: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[0;32m    113\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    114\u001b[0m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[0;32m    115\u001b[0m ):\n\u001b[1;32m--> 116\u001b[0m     wait_fn(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         spin_cb()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_current_time(*args, **kwargs):\n",
    "    import datetime\n",
    "    now= datetime.datetime.now()\n",
    "    return now.strftime(\"%I:%M %p\")\n",
    "\n",
    "tools= [\n",
    "    Tool(\n",
    "        name= \"Time\",\n",
    "        func= get_current_time,\n",
    "        description= \"Useful when you need to know the current time\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt= hub.pull(\"hwchase17/react\")\n",
    "'''\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "'''\n",
    "\n",
    "agent= create_react_agent(\n",
    "    llm= llm,\n",
    "    tools= tools,\n",
    "    prompt= prompt,\n",
    "    stop_sequence= True\n",
    ")\n",
    "\n",
    "agent_executor= AgentExecutor.from_agent_and_tools(\n",
    "    agent= agent,\n",
    "    tools= tools,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "response= agent_executor.invoke({\"input\": \"What time it is?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee16a2f-8f2c-42c4-bc02-e9a56b4dd372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
